{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------------\n",
    "## Summary : perform parsing using tree parser\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------------------\n",
    "\n",
    "# paper : http://aclweb.org/anthology/P/P15/P15-1033.pdf\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from itertools import count\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# actions we can take\n",
    "SHIFT = 0\n",
    "REDUCE_L = 1\n",
    "REDUCE_R = 2\n",
    "\n",
    "NUM_ACTIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class to read the vocabulary\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_list(cls, words):\n",
    "        w2i = {}\n",
    "        idx = 0\n",
    "        \n",
    "        for word in words:\n",
    "            w2i[word] = idx\n",
    "            idx += 1\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, vocab_fname):\n",
    "        words = []\n",
    "        with open(vocab_fname) as fh:\n",
    "            for line in fh:\n",
    "                line.strip()\n",
    "                word, count = line.split()\n",
    "                words.append(word)\n",
    "        return Vocab.from_list(words)\n",
    "\n",
    "    def size(self): \n",
    "        return len(self.w2i.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets define a method to read the data\n",
    "def read_oracle(fname, vw, va):\n",
    "    with open(fname) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            ssent, sacts = re.split(r' \\|\\|\\| ', line)\n",
    "            sent = [vw.w2i[x] for x in ssent.split()]\n",
    "            acts = [va.w2i[x] for x in sacts.split()]\n",
    "            yield (sent, acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_DIM = 64\n",
    "LSTM_DIM = 64\n",
    "ACTION_DIM = 32\n",
    "\n",
    "class TransitionParser:\n",
    "\n",
    "    def __init__(self, model, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.pW_comp = model.add_parameters((LSTM_DIM, LSTM_DIM * 2))\n",
    "        self.pb_comp = model.add_parameters((LSTM_DIM, ))\n",
    "        self.pW_s2h = model.add_parameters((LSTM_DIM, LSTM_DIM * 2))\n",
    "        self.pb_s2h = model.add_parameters((LSTM_DIM, ))\n",
    "        self.pW_act = model.add_parameters((NUM_ACTIONS, LSTM_DIM))\n",
    "        self.pb_act = model.add_parameters((NUM_ACTIONS, ))\n",
    "        \n",
    "        # layers, in-dim, out-dim, model\n",
    "        self.buffRNN = dy.LSTMBuilder(1, WORD_DIM, LSTM_DIM, model)\n",
    "        self.stackRNN = dy.LSTMBuilder(1, WORD_DIM, LSTM_DIM, model)\n",
    "        self.pempty_buffer_emb = model.add_parameters((LSTM_DIM,))\n",
    "        nwords=vocab.size()\n",
    "        self.WORDS_LOOKUP = model.add_lookup_parameters((nwords, WORD_DIM))\n",
    "        \n",
    "    \n",
    "    # returns an expression of the loss for the sequence of actions\n",
    "    # (that is, the oracle_actions if present or the predicted sequence otherwise)\n",
    "    def parse(self, t, oracle_actions=None):\n",
    "        dy.renew_cg()\n",
    "        \n",
    "        if oracle_actions:\n",
    "            oracle_actions = list(oracle_actions)\n",
    "            oracle_actions.reverse()\n",
    "        \n",
    "        stack_top = self.stackRNN.initial_state()\n",
    "        toks = list(t)\n",
    "        toks.reverse()\n",
    "        stack = []\n",
    "        \n",
    "        cur = self.buffRNN.initial_state()\n",
    "        buffer = []\n",
    "        empty_buffer_emb = dy.parameter(self.pempty_buffer_emb)\n",
    "\n",
    "        W_comp = dy.parameter(self.pW_comp)\n",
    "        b_comp = dy.parameter(self.pb_comp)\n",
    "        W_s2h = dy.parameter(self.pW_s2h)\n",
    "        b_s2h = dy.parameter(self.pb_s2h)\n",
    "\n",
    "        W_act = dy.parameter(self.pW_act)\n",
    "        b_act = dy.parameter(self.pb_act)\n",
    "\n",
    "        losses = []\n",
    "        for tok in toks:\n",
    "            tok_embedding = self.WORDS_LOOKUP[tok]\n",
    "            cur = cur.add_input(tok_embedding)\n",
    "            buffer.append((cur.output(), tok_embedding, self.vocab.i2w[tok]))\n",
    "\n",
    "        while not (len(stack) == 1 and len(buffer) == 0):\n",
    "            # based on parser state, get valid actions\n",
    "            valid_actions = []\n",
    "            if len(buffer) > 0:  # can only reduce if elements in buffer\n",
    "                valid_actions += [SHIFT]\n",
    "            if len(stack) >= 2:  # can only shift if 2 elements on stack\n",
    "                valid_actions += [REDUCE_L, REDUCE_R]\n",
    "\n",
    "            # compute probability of each of the actions and choose an action\n",
    "            # either from the oracle or if there is no oracle, based on the model\n",
    "            action = valid_actions[0]\n",
    "            log_probs = None\n",
    "\n",
    "            if len(valid_actions) > 1:\n",
    "                buffer_embedding = buffer[-1][0] if buffer else empty_buffer_emb\n",
    "                stack_embedding = stack[-1][0].output() # the stack has something here\n",
    "                parser_state = dy.concatenate([buffer_embedding, stack_embedding])\n",
    "                \n",
    "                h = dy.tanh(W_s2h * parser_state + b_s2h)\n",
    "                logits = W_act * h + b_act\n",
    "                log_probs = dy.log_softmax(logits, valid_actions)\n",
    "                \n",
    "                if oracle_actions is None:\n",
    "                    action = max(enumerate(log_probs.vec_value()), key=itemgetter(1))[0]\n",
    "\n",
    "                if oracle_actions is not None:\n",
    "                    action = oracle_actions.pop()\n",
    "                    if log_probs is not None:\n",
    "                        # append the action-specific loss\n",
    "                        losses.append(dy.pick(log_probs, action))\n",
    "\n",
    "                # execute the action to update the parser state\n",
    "                if action == SHIFT:\n",
    "                    _, tok_embedding, token = buffer.pop()\n",
    "                    stack_state, _ = stack[-1] if stack else (stack_top, '<TOP>')\n",
    "                    stack_state = stack_state.add_input(tok_embedding)\n",
    "                    stack.append((stack_state, token))\n",
    "                \n",
    "                else: # one of the reduce actions\n",
    "                    right = stack.pop()\n",
    "                    left = stack.pop()\n",
    "                    head, modifier = (left, right) if action == REDUCE_R else (right, left)\n",
    "                    top_stack_state, _ = stack[-1] if stack else (stack_top, '<TOP>')\n",
    "                    head_rep, head_tok = head[0].output(), head[1]\n",
    "                    mod_rep, mod_tok = modifier[0].output(), modifier[1]\n",
    "                    composed_rep = dy.rectify(W_comp * dy.concatenate([head_rep, mod_rep]) + b_comp)\n",
    "                    top_stack_state = top_stack_state.add_input(composed_rep)\n",
    "                    stack.append((top_stack_state, head_tok))\n",
    "                    if oracle_actions is None:\n",
    "                        print('{0} --> {1}'.format(head_tok, mod_tok))\n",
    "\n",
    "        # the head of the tree that remains at the top of the stack is now the root\n",
    "        if oracle_actions is None:\n",
    "            head = stack.pop()[1]\n",
    "            print('ROOT --> {0}'.format(head))\n",
    "\n",
    "        return -dy.esum(losses) if losses else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acts = ['SHIFT', 'REDUCE_L', 'REDUCE_R']\n",
    "vocab_acts = Vocab.from_list(acts)\n",
    "\n",
    "vocab_words = Vocab.from_file('../data/parsing/shift_reduce/vocab.txt')\n",
    "train = list(read_oracle('../data/parsing/shift_reduce/small-train.unk.txt', vocab_words, vocab_acts))\n",
    "dev = list(read_oracle('../data/parsing/shift_reduce/small-dev.unk.txt', vocab_words, vocab_acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model and trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "tp = TransitionParser(model, vocab_words)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for epoch in range(5):\n",
    "    words = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for (s,a) in train:\n",
    "        loss = tp.parse(s, a)\n",
    "        words += len(s)\n",
    "        if loss is not None:\n",
    "            total_loss += loss.scalar_value()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "        e = float(i) / len(train)\n",
    "        if i % 50 == 0:\n",
    "            print('epoch {}: per-word loss: {}'.format(e, total_loss / words))\n",
    "            words = 0\n",
    "            total_loss = 0.0\n",
    "        if i % 500 == 0:\n",
    "            tp.parse(dev[209][0])\n",
    "            dev_words = 0\n",
    "            dev_loss = 0.0\n",
    "            for (ds, da) in dev:\n",
    "                loss = tp.parse(ds, da)\n",
    "                dev_words += len(ds)\n",
    "                if loss is not None:\n",
    "                    dev_loss += loss.scalar_value()\n",
    "        print('[validation] epoch {}: per-word loss: {}'.format(e, dev_loss / dev_words))\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
