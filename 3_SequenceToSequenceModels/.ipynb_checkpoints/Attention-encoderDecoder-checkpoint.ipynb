{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------------\n",
    "## Summary : Perform language translation using LSTM encoder decoder with attention\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets convert word to integer. Since source and target languages are different\n",
    "# we use two different converters\n",
    "\n",
    "w2i_src = defaultdict(lambda: len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
    "\n",
    "\n",
    "# insert UNK and eos into both languages\n",
    "UNK_SRC = w2i_src['<unk>']\n",
    "UNK_TRG = w2i_trg['<unk>']\n",
    "EOS_SRC = w2i_src['</s>']\n",
    "EOS_TRG = w2i_trg['</s>']\n",
    "SOS_TRG = w2i_trg['<s>']\n",
    "\n",
    "\n",
    "# lets write a method to read the sentences from two languages at the same time\n",
    "def readDataset(srcFileName,trgFileName):\n",
    "    retList = []\n",
    "    \n",
    "    with open(srcFileName,'r+',encoding='utf8') as src, open(trgFileName,'r+',encoding='utf8') as trg:\n",
    "        for srcLine,trgLine in zip(src,trg):\n",
    "            srcWords = [w2i_src[x] for x in srcLine.lower().strip().split()+['</s>']]\n",
    "            trgWords = [w2i_trg[x] for x in ['<s>']+trgLine.strip().split()+['</s>']]\n",
    "            retList.append([srcWords,trgWords])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8059  :  7043\n"
     ]
    }
   ],
   "source": [
    "# read the train dataSet\n",
    "train = readDataset('../data/parallel/train.ja','../data/parallel/train.en')\n",
    "w2i_src = defaultdict(lambda: UNK_SRC, w2i_src)             # when the item is not present it is treated as UNK\n",
    "w2i_trg = defaultdict(lambda: UNK_TRG, w2i_trg)             # when the item is not present it is treated as UNK\n",
    "\n",
    "nWordsSrc = len(w2i_src)\n",
    "nWordsTrg = len(w2i_trg)\n",
    "\n",
    "print(nWordsSrc,' : ',nWordsTrg)\n",
    "\n",
    "# read dev and test data \n",
    "dev = readDataset('../data/parallel/dev.ja','../data/parallel/dev.en')\n",
    "test = readDataset('../data/parallel/test.ja','../data/parallel/test.en')\n",
    "\n",
    "# method to convert the integers back to words\n",
    "i2w_trg = {v: k for k, v in w2i_trg.items()}\n",
    "i2w_src = {v: k for k, v in w2i_src.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "EMBED_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "ATTENTION_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# During the early stages of training and sometimes in later stages of training the model\n",
    "# can generate very long sentences. To prevent that put a max limit on the sentence size\n",
    "MAX_SENT_SIZE = 50\n",
    "\n",
    "\n",
    "# lets declare the model and trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# lets declare the parameters\n",
    "# lookup params for the words\n",
    "W_emb_src = model.add_lookup_parameters((nWordsSrc,EMBED_SIZE))\n",
    "W_emb_trg = model.add_lookup_parameters((nWordsTrg,EMBED_SIZE))\n",
    "\n",
    "# LSTMS for the encoders and decoders. We are using the BiDirectional LSTM for source and\n",
    "# normal one for decoder\n",
    "lstmSrc = dy.BiRNNBuilder(1,EMBED_SIZE,HIDDEN_SIZE,model,dy.LSTMBuilder)\n",
    "lstmTrg = dy.LSTMBuilder(1,EMBED_SIZE,HIDDEN_SIZE,model)\n",
    "\n",
    "# weights for layer before the softmax layer. This is additional component for attention\n",
    "W_Atten = model.add_parameters((HIDDEN_SIZE,2*HIDDEN_SIZE))\n",
    "b_Atten = model.add_parameters((HIDDEN_SIZE))\n",
    "\n",
    "# softmax weights\n",
    "W_sm = model.add_parameters((nWordsTrg,HIDDEN_SIZE))\n",
    "b_sm = model.add_parameters((nWordsTrg))\n",
    "\n",
    "# attention weights\n",
    "w1_att_src_p = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "w1_att_tgt_p = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "w2_att_p = model.add_parameters((ATTENTION_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to compute the attention given source output matrix,\n",
    "# target embedding and fixed component\n",
    "# we are using MLP w2*tanh(W1*(q,k))\n",
    "def calc_attention(src_output_matrix,trg_out_embedding,fixed_component):\n",
    "    # get the attention weights to computation graph\n",
    "    w1_Trg = dy.parameter(w1_att_tgt_p)\n",
    "    w2_att = dy.parameter(w2_att_p)\n",
    "    \n",
    "    attention = dy.transpose(dy.tanh(dy.colwise_add(fixed_component,w1_Trg*trg_out_embedding)))*w2_att\n",
    "    alignment = dy.softmax(attention)\n",
    "    attention = src_output_matrix*alignment\n",
    "    return attention,alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to compute the loss\n",
    "def computeLoss(sents):\n",
    "    # renew the computation graph\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # get the source and target sentences\n",
    "    src_sents = [x[0] for x in sents]\n",
    "    trg_sents = [x[1] for x in sents]\n",
    "    src_cws = []\n",
    "\n",
    "    # get the length of source sentence \n",
    "    len_src = [len(x) for x in src_sents]\n",
    "    max_src_len = np.max(len_src)\n",
    "    num_words = 0\n",
    "    \n",
    "    # get all the ith words aligned\n",
    "    for i in range(max_src_len):\n",
    "        src_cws.append([sent[i] for sent in src_sents])\n",
    "        \n",
    "    # get the output of the encoder LSTM\n",
    "    src_outputs = [dy.concatenate([x.output(),y.output()]) for x,y in lstmSrc.add_inputs([dy.lookup_batch(W_emb_src,cws) for cws in src_cws])]\n",
    "    src_output = src_outputs[-1]          # output of encoder\n",
    "    \n",
    "    # get the fixed component to caluclate the attention\n",
    "    src_output_matrix = dy.concatenate_cols(src_outputs)\n",
    "    w1_att = dy.parameter(w1_att_src_p)\n",
    "    fixed_component = w1_att * src_output_matrix\n",
    "    \n",
    "    # lets do the decoder part now\n",
    "    allLosses = []\n",
    "    \n",
    "    # we need to get the mask and get the target words\n",
    "    len_trg = [len(x) for x in trg_sents]\n",
    "    max_trg_len = np.max(len_trg)\n",
    "    trg_cws = []\n",
    "    masks = []\n",
    "    \n",
    "    for i in range(max_trg_len):\n",
    "        trg_cws.append([sent[i] if len(sent) > i else EOS_TRG for sent in trg_sents])\n",
    "        mask = [(1 if len(sent)>i else 0) for sent in trg_sents]\n",
    "        masks.append(mask)\n",
    "        num_words += sum(mask)\n",
    "        \n",
    "    # initialize the current state of the decoder\n",
    "    cur_state = lstmTrg.initial_state().set_s([src_output,dy.tanh(src_output)])\n",
    "    prev_words = trg_cws[0]\n",
    "    \n",
    "    # get the parameters to Computation Graph\n",
    "    weightsAtten = dy.parameter(W_Atten)\n",
    "    biasesAtten = dy.parameter(b_Atten)\n",
    "    weightsSm = dy.parameter(W_sm)\n",
    "    biasesSm = dy.parameter(b_sm)\n",
    "        \n",
    "    for next_words,mask in zip(trg_cws[1:],masks):\n",
    "        # feed the current state into the network and get the output\n",
    "        cur_state = cur_state.add_input(dy.lookup_batch(W_emb_trg,prev_words))\n",
    "        output_emb = cur_state.output()\n",
    "        attention,alignment = calc_attention(src_output_matrix,output_emb,fixed_component)\n",
    "        middle_expr = dy.tanh(dy.affine_transform([biasesAtten,weightsAtten,dy.concatenate([output_emb,attention])]))\n",
    "        \n",
    "        # get the scores and compute the loss\n",
    "        s = dy.affine_transform([biasesSm,weightsSm,middle_expr])\n",
    "        loss = (dy.pickneglogsoftmax_batch(s,next_words))\n",
    "        \n",
    "        # get the mask\n",
    "        mask_expr = dy.inputVector(mask)\n",
    "        mask_expr = dy.reshape(mask_expr, (1,),len(sents))\n",
    "        \n",
    "        # compute the loss\n",
    "        mask_loss = loss * mask_expr\n",
    "        allLosses.append(mask_loss)\n",
    "        prev_words = next_words\n",
    "    return dy.sum_batches(dy.esum(allLosses)), num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now lets write a method to translate the sentence\n",
    "def translate(sent):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # get the output of encoder\n",
    "    src_outputs = [dy.concatenate([x.output(), y.output()]) for x,y in lstmSrc.add_inputs([W_emb_src[word] for word in sent])]\n",
    "    src_output = src_outputs[-1]          # output of encoder\n",
    "    \n",
    "    # get the fixed component to caluclate the attention\n",
    "    src_output_matrix = dy.concatenate_cols(src_outputs)\n",
    "    w1_att = dy.parameter(w1_att_src_p)\n",
    "    fixed_component = w1_att * src_output_matrix\n",
    "    \n",
    "    \n",
    "    # initialize the current state of the decoder\n",
    "    cur_state = lstmTrg.initial_state().set_s([src_output,dy.tanh(src_output)])\n",
    "    \n",
    "    # initialize the source sent to SOS_TRG\n",
    "    prev_word = SOS_TRG\n",
    "    trg_sent = []\n",
    "    attention_matrix = []\n",
    "    \n",
    "    # get the parameters to Computation Graph\n",
    "    weightsAtten = dy.parameter(W_Atten)\n",
    "    biasesAtten = dy.parameter(b_Atten)\n",
    "    weightsSm = dy.parameter(W_sm)\n",
    "    biasesSm = dy.parameter(b_sm)\n",
    "    \n",
    "    # generate the sentences while sentence length is less than max_sent_length\n",
    "    for i in range(MAX_SENT_SIZE):\n",
    "        cur_state = cur_state.add_input(W_emb_trg[prev_word])\n",
    "        output_emb = cur_state.output()\n",
    "        attention,alignment = calc_attention(src_output_matrix,output_emb,fixed_component)\n",
    "        attention_matrix.append(alignment)\n",
    "        middle_expr = dy.tanh(dy.affine_transform([biasesAtten,weightsAtten,dy.concatenate([output_emb,attention])]))\n",
    "        \n",
    "        # get the scores\n",
    "        s = dy.affine_transform([biasesSm,weightsSm,middle_expr]).value()\n",
    "        next_word = np.argmax(s)\n",
    "        \n",
    "        if next_word == EOS_TRG:\n",
    "            break\n",
    "        prev_word = next_word\n",
    "        trg_sent.append(i2w_trg[next_word])\n",
    "    return trg_sent,dy.concatenate_cols(attention_matrix).value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to get the data into batches\n",
    "def create_batches(sorted_dataset, max_batch_size):\n",
    "    source = [x[0] for x in sorted_dataset]\n",
    "    src_lengths = [len(x) for x in source]\n",
    "    batches = []\n",
    "    prev = src_lengths[0]\n",
    "    prev_start = 0\n",
    "    batch_size = 1\n",
    "    \n",
    "    for i in range(1, len(src_lengths)):\n",
    "        if src_lengths[i] != prev or batch_size == max_batch_size:\n",
    "            batches.append((prev_start, batch_size))\n",
    "            prev = src_lengths[i]\n",
    "            prev_start = i\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size += 1\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training.....\n",
      "Training loss per word at iteration :  0  is  5.027200011916914 . Time taken :  107.88814067840576\n",
      "Dev loss per word at iteration :  0  is  5.085063047139328 . Time taken :  2.4066691398620605\n",
      "Training loss per word at iteration :  1  is  4.330869026744074 . Time taken :  107.15615224838257\n",
      "Dev loss per word at iteration :  1  is  4.7745328823138395 . Time taken :  2.3922231197357178\n",
      "Training loss per word at iteration :  2  is  4.015895939109432 . Time taken :  107.11620306968689\n",
      "Dev loss per word at iteration :  2  is  4.542926129296629 . Time taken :  2.4305260181427\n",
      "Training loss per word at iteration :  3  is  3.7655969351224354 . Time taken :  108.68468761444092\n",
      "Dev loss per word at iteration :  3  is  4.575525460769799 . Time taken :  2.396254301071167\n",
      "Training loss per word at iteration :  4  is  3.5617951850061242 . Time taken :  107.58853387832642\n",
      "Dev loss per word at iteration :  4  is  4.522007629004197 . Time taken :  2.4098799228668213\n",
      "Training loss per word at iteration :  5  is  3.387318948017737 . Time taken :  107.21590757369995\n",
      "Dev loss per word at iteration :  5  is  4.479066374924264 . Time taken :  2.4103565216064453\n",
      "Training loss per word at iteration :  6  is  3.2263561824121196 . Time taken :  109.5436429977417\n",
      "Dev loss per word at iteration :  6  is  4.434837293203962 . Time taken :  2.4052047729492188\n",
      "Training loss per word at iteration :  7  is  3.078654153953393 . Time taken :  106.93104434013367\n",
      "Dev loss per word at iteration :  7  is  4.412613797415443 . Time taken :  2.407904624938965\n",
      "Training loss per word at iteration :  8  is  2.9357012261929425 . Time taken :  107.06060481071472\n",
      "Dev loss per word at iteration :  8  is  4.419599437902606 . Time taken :  2.4408977031707764\n",
      "Training loss per word at iteration :  9  is  2.8027137945260856 . Time taken :  107.43351721763611\n",
      "Dev loss per word at iteration :  9  is  4.439770191558909 . Time taken :  2.4026026725769043\n"
     ]
    }
   ],
   "source": [
    "# lets start the training\n",
    "print('started training.....')\n",
    "\n",
    "for i in range(10):\n",
    "    # Perform training\n",
    "    train.sort(key=lambda t: len(t[0]), reverse=True)\n",
    "    dev.sort(key=lambda t: len(t[0]), reverse=True)\n",
    "    \n",
    "    train_order = create_batches(train, BATCH_SIZE)\n",
    "    dev_order = create_batches(dev, BATCH_SIZE)\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    startTime = time.time()\n",
    "    \n",
    "    for sent_id, (start, length) in enumerate(train_order):\n",
    "        train_batch = train[start:start+length]\n",
    "        my_loss, num_words = computeLoss(train_batch)\n",
    "        train_loss += my_loss.value()\n",
    "        train_words += num_words\n",
    "        my_loss.backward()\n",
    "        trainer.update()    \n",
    "    print(\"Training loss per word at iteration : \",i,\" is \",train_loss/train_words,\". Time taken : \",(-startTime+time.time()))\n",
    "    \n",
    "    startTime = time.time()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    for sent_id, (start, length) in enumerate(dev_order):\n",
    "        dev_batch = dev[start:start+length]\n",
    "        my_loss, num_words = computeLoss(dev_batch)\n",
    "        dev_loss += my_loss.value()\n",
    "        dev_words += num_words\n",
    "    print(\"Dev loss per word at iteration : \",i,\" is \",dev_loss/dev_words,\". Time taken : \",(-startTime+time.time()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original Translation :  ['<s>', 'can', 'you', 'do', 'it', 'in', 'one', 'day', '?', '</s>']\n",
      "obtained Translation :  ['do', 'you', 'have', 'a', 'drink', '?']\n",
      "original Translation :  ['<s>', 'he', 'stared', 'at', 'me', 'with', 'a', 'dating', 'smile', '.', '</s>']\n",
      "obtained Translation :  ['i', 'got', 'a', 'lot', 'of', 'breath', '.']\n",
      "original Translation :  ['<s>', 'it', '&apos;s', 'time', 'to', 'leave', '.', '</s>']\n",
      "obtained Translation :  ['we', 'are', 'my', 'regular', 'change', '.']\n"
     ]
    }
   ],
   "source": [
    "# lets generate some sentences\n",
    "src = test[0][0]\n",
    "trg = test[0][1]\n",
    "\n",
    "output_sent, attention_matrix = translate(src)\n",
    "print('original Translation : ',[i2w_trg[x] for x in trg])\n",
    "print('obtained Translation : ',output_sent)\n",
    "\n",
    "\n",
    "# lets generate some sentences\n",
    "src = test[1][0]\n",
    "trg = test[1][1]\n",
    "\n",
    "output_sent, attention_matrix = translate(src)\n",
    "print('original Translation : ',[i2w_trg[x] for x in trg])\n",
    "print('obtained Translation : ',output_sent)\n",
    "\n",
    "\n",
    "# lets generate some sentences\n",
    "src = test[2][0]\n",
    "trg = test[2][1]\n",
    "\n",
    "output_sent, attention_matrix = translate(src)\n",
    "print('original Translation : ',[i2w_trg[x] for x in trg])\n",
    "print('obtained Translation : ',output_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
