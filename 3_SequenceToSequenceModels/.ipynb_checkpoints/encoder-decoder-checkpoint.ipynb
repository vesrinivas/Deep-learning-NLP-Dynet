{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------------\n",
    "## Summary : Perform language translation using LSTM encoder decoder\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets convert word to integer. Since source and target languages are different\n",
    "# we use two different converters\n",
    "w2i_src = defaultdict(lambda: len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
    "\n",
    "\n",
    "# lets write a method to read the sentences from two languages at the same time\n",
    "def readDataset(srcFileName,trgFileName):\n",
    "    retList = []\n",
    "    \n",
    "    with open(srcFileName,'r+',encoding='utf8') as src, open(trgFileName,'r+',encoding='utf8') as trg:\n",
    "        for srcLine,trgLine in zip(src,trg):\n",
    "            srcWords = [w2i_src[x] for x in srcLine.lower().strip().split()]\n",
    "            trgWords = [w2i_trg[x] for x in trgLine.strip().split()]\n",
    "            retList.append([srcWords,trgWords])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of japanese words :  8059\n",
      "number of english words :  7043\n"
     ]
    }
   ],
   "source": [
    "# read the train and test dataSets\n",
    "train = readDataset('../data/parallel/train.ja','../data/parallel/train.en')\n",
    "\n",
    "# insert UNK and eos into both languages\n",
    "UNK_SRC = w2i_src['<unk>']\n",
    "UNK_TRG = w2i_trg['<unk>']\n",
    "EOS_SRC = w2i_src['</s>']\n",
    "EOS_TRG = w2i_trg['</s>']\n",
    "SOS_TRG = w2i_trg['<s>']\n",
    "\n",
    "nWords_src = len(w2i_src)\n",
    "nWords_trg = len(w2i_trg)\n",
    "\n",
    "\n",
    "print('number of japanese words : ',nWords_src)\n",
    "print('number of english words : ',nWords_trg)\n",
    "\n",
    "w2i_src = defaultdict(lambda: UNK_SRC, w2i_src)\n",
    "w2i_trg = defaultdict(lambda: UNK_TRG, w2i_trg)\n",
    "\n",
    "\n",
    "# lets write a method to convert the integers back to words\n",
    "i2w_trg = {v:k for k,v in w2i_trg.items()}\n",
    "\n",
    "dev = readDataset('../data/parallel/dev.ja','../data/parallel/dev.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets declare the embedding size, Hidden layer size\n",
    "nEmb = 64\n",
    "nHid = 128\n",
    "\n",
    "# lets declare the model and trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# lets declare the parameters we use in the network\n",
    "# embeddings\n",
    "W_emb_src = model.add_lookup_parameters((nWords_src,nEmb))\n",
    "W_emb_trg = model.add_lookup_parameters(((nWords_trg,nEmb)))\n",
    "\n",
    "# LSTMS for encoder and decoder\n",
    "lstmEncoder = dy.LSTMBuilder(1,nEmb,nHid,model)\n",
    "lstmDecoder = dy.LSTMBuilder(1,nEmb,nHid,model)\n",
    "\n",
    "# softmax weights\n",
    "W_sm = model.add_parameters((nWords_trg,nHid))\n",
    "b_sm = model.add_parameters((nWords_trg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to compute the loss given a batch of sentences\n",
    "def computeLoss(sent):\n",
    "    # renew the computation graph\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # initialize allLosses and get softmax weights and biases to computation graph\n",
    "    allLosses = []\n",
    "    weightsSoftMax = dy.parameter(W_sm)\n",
    "    biasesSoftMax = dy.parameter(b_sm)\n",
    "        \n",
    "    # get the source and target\n",
    "    src = sent[0]\n",
    "    trg = sent[1]\n",
    "    \n",
    "    # initialize the encoder LSTM\n",
    "    enc_init = lstmEncoder.initial_state()\n",
    "    \n",
    "    # pass the source sent through the encoder get the encoder output\n",
    "    enc_out = enc_init.add_inputs([W_emb_src[x] for x in src])[-1].output()\n",
    "    \n",
    "    # now lets initialize the decoder output and set it enc_out and hidden to tanh(enc_out)\n",
    "    dec_curState = lstmDecoder.initial_state().set_s([enc_out,dy.tanh(enc_out)])\n",
    "    \n",
    "   \n",
    "    prev_word = trg[0]\n",
    "    # now iterate through every word in the sent and compute the loss\n",
    "    for next_word in trg[1:]:\n",
    "        # feed the current word to the decoder\n",
    "        dec_curState = dec_curState.add_input(W_emb_trg[prev_word])\n",
    "        output = dec_curState.output()\n",
    "        \n",
    "        # compute the softmax output\n",
    "        softMaxOut = dy.affine_transform([biasesSoftMax,weightsSoftMax,output]) \n",
    "        \n",
    "        # compute the loss and update the losses\n",
    "        allLosses.append(dy.pickneglogsoftmax(softMaxOut, next_word))\n",
    "        \n",
    "        # update the prev_word\n",
    "        prev_word = next_word\n",
    "        \n",
    "    return dy.esum(allLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets declare the maximum sentence size\n",
    "MAX_SENT_SIZE = 100\n",
    "\n",
    "# now lets write a method to find the translation\n",
    "def translate(sent):\n",
    "    # renew the computation graph\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # get the weights and biases of softmax to the computation graph\n",
    "    weightsSoftmax = dy.parameter(W_sm)\n",
    "    biasesSoftmax = dy.parameter(b_sm)\n",
    "    \n",
    "    # pass the input sentence throught the encoder\n",
    "    enc_init = lstmEncoder.initial_state()\n",
    "    enc_out = enc_init.add_inputs([W_emb_src[x] for x in sent])[-1].output()\n",
    "    \n",
    "    # now lets initialize the decoder output and set it enc_out and hidden to tanh(enc_out)\n",
    "    dec_curState = lstmDecoder.initial_state().set_s([enc_out,dy.tanh(enc_out)])\n",
    "    \n",
    "    # lets declare the first word to be SOS\n",
    "    prev_word = SOS_TRG\n",
    "    retSent = []\n",
    "    \n",
    "    for i in range(MAX_SENT_SIZE):\n",
    "        dec_curState = dec_curState.add_input(W_emb_trg[prev_word])\n",
    "        output = dec_curState.output()\n",
    "        \n",
    "        # compute the softmax output\n",
    "        softMaxOut = dy.affine_transform([biasesSoftmax,weightsSoftmax,output]) \n",
    "        \n",
    "        # compute probabilities of each word and choose the word with highest probability\n",
    "        # prob = dy.log_softmax(softMaxOut).npvalue()\n",
    "        next_word = np.argmax(softMaxOut.npvalue())\n",
    "        \n",
    "        if next_word == EOS_TRG:\n",
    "            break\n",
    "        \n",
    "        # update the prev_word\n",
    "        prev_word = next_word\n",
    "        retSent.append(i2w_trg[next_word])\n",
    "    return retSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training.....\n",
      "Training loss per sentence at iteration :  0  is  48.786718786621094 . Time taken :  468.01820969581604\n",
      "Training loss per sentence at iteration :  0  is  43.86825667572022 . Time taken :  22.597745656967163\n",
      "Training loss per sentence at iteration :  1  is  41.67604865269661 . Time taken :  457.0389778614044\n",
      "Training loss per sentence at iteration :  1  is  38.1275664434433 . Time taken :  22.603434801101685\n",
      "Training loss per sentence at iteration :  2  is  38.42163778848648 . Time taken :  462.5360805988312\n",
      "Training loss per sentence at iteration :  2  is  34.869053071975706 . Time taken :  22.546091079711914\n",
      "Training loss per sentence at iteration :  3  is  35.774328156399726 . Time taken :  464.0526051521301\n",
      "Training loss per sentence at iteration :  3  is  32.34425690937042 . Time taken :  22.766379594802856\n",
      "Training loss per sentence at iteration :  4  is  33.42880968849659 . Time taken :  465.3874969482422\n",
      "Training loss per sentence at iteration :  4  is  30.345897130966186 . Time taken :  22.70893907546997\n",
      "Training loss per sentence at iteration :  5  is  31.230472750782965 . Time taken :  458.7828664779663\n",
      "Training loss per sentence at iteration :  5  is  28.37770883655548 . Time taken :  22.501976251602173\n",
      "Training loss per sentence at iteration :  6  is  29.19321652817726 . Time taken :  461.8838176727295\n",
      "Training loss per sentence at iteration :  6  is  26.766984159946443 . Time taken :  22.546979427337646\n",
      "Training loss per sentence at iteration :  7  is  27.336001459503173 . Time taken :  459.39403557777405\n",
      "Training loss per sentence at iteration :  7  is  25.106131429672242 . Time taken :  22.6242036819458\n",
      "Training loss per sentence at iteration :  8  is  25.78849713615179 . Time taken :  504.2679615020752\n",
      "Training loss per sentence at iteration :  8  is  23.69458857870102 . Time taken :  26.013567209243774\n",
      "Training loss per sentence at iteration :  9  is  24.374215912771223 . Time taken :  474.07469272613525\n",
      "Training loss per sentence at iteration :  9  is  22.632738807916642 . Time taken :  22.557990550994873\n",
      "Training loss per sentence at iteration :  10  is  23.060157384741306 . Time taken :  244.63817477226257\n",
      "Training loss per sentence at iteration :  10  is  21.496003143072127 . Time taken :  11.15604543685913\n",
      "Training loss per sentence at iteration :  11  is  21.926362757718564 . Time taken :  233.9671449661255\n",
      "Training loss per sentence at iteration :  11  is  20.712673630952835 . Time taken :  11.015599727630615\n",
      "Training loss per sentence at iteration :  12  is  20.857872823023797 . Time taken :  225.41714477539062\n",
      "Training loss per sentence at iteration :  12  is  19.841818270772695 . Time taken :  10.968681812286377\n",
      "Training loss per sentence at iteration :  13  is  19.86823741682768 . Time taken :  223.9470784664154\n",
      "Training loss per sentence at iteration :  13  is  19.16075884383917 . Time taken :  10.953126907348633\n",
      "Training loss per sentence at iteration :  14  is  19.019238874666392 . Time taken :  224.60353827476501\n",
      "Training loss per sentence at iteration :  14  is  18.430308287143706 . Time taken :  11.015566110610962\n",
      "Training loss per sentence at iteration :  15  is  18.24059447484389 . Time taken :  226.63676118850708\n",
      "Training loss per sentence at iteration :  15  is  17.973095178961753 . Time taken :  11.23264217376709\n",
      "Training loss per sentence at iteration :  16  is  17.514615207591653 . Time taken :  223.71662259101868\n",
      "Training loss per sentence at iteration :  16  is  17.329948725819587 . Time taken :  10.99998950958252\n",
      "Training loss per sentence at iteration :  17  is  16.880329427344353 . Time taken :  225.20277571678162\n",
      "Training loss per sentence at iteration :  17  is  16.864732637763023 . Time taken :  11.015671968460083\n",
      "Training loss per sentence at iteration :  18  is  16.282712242123857 . Time taken :  224.28658938407898\n",
      "Training loss per sentence at iteration :  18  is  16.309915398776532 . Time taken :  11.031181335449219\n",
      "Training loss per sentence at iteration :  19  is  15.721161193482018 . Time taken :  225.7028317451477\n",
      "Training loss per sentence at iteration :  19  is  15.903687369048596 . Time taken :  11.093726634979248\n",
      "Training loss per sentence at iteration :  20  is  15.207240170728042 . Time taken :  228.84617948532104\n",
      "Training loss per sentence at iteration :  20  is  15.605112318575383 . Time taken :  11.75312066078186\n",
      "Training loss per sentence at iteration :  21  is  14.780710555743426 . Time taken :  240.56209683418274\n",
      "Training loss per sentence at iteration :  21  is  15.162660206079483 . Time taken :  12.296859741210938\n",
      "Training loss per sentence at iteration :  22  is  14.384248151639104 . Time taken :  225.07781147956848\n",
      "Training loss per sentence at iteration :  22  is  15.264646715074777 . Time taken :  11.15617060661316\n",
      "Training loss per sentence at iteration :  23  is  14.009645328551532 . Time taken :  226.19809865951538\n",
      "Training loss per sentence at iteration :  23  is  14.805450298517943 . Time taken :  10.98436427116394\n",
      "Training loss per sentence at iteration :  24  is  13.644343405080587 . Time taken :  223.77900981903076\n",
      "Training loss per sentence at iteration :  24  is  14.701965158611536 . Time taken :  11.015606880187988\n",
      "Training loss per sentence at iteration :  25  is  13.380815671427548 . Time taken :  223.68177485466003\n",
      "Training loss per sentence at iteration :  25  is  14.258928748190403 . Time taken :  11.015612840652466\n",
      "Training loss per sentence at iteration :  26  is  13.065242325153202 . Time taken :  225.71846866607666\n",
      "Training loss per sentence at iteration :  26  is  13.946610646903515 . Time taken :  11.109289646148682\n",
      "Training loss per sentence at iteration :  27  is  12.850912100881525 . Time taken :  228.18720984458923\n",
      "Training loss per sentence at iteration :  27  is  13.926664698302746 . Time taken :  11.187435865402222\n",
      "Training loss per sentence at iteration :  28  is  12.586487959272414 . Time taken :  225.23004603385925\n",
      "Training loss per sentence at iteration :  28  is  13.80112766355276 . Time taken :  11.029433012008667\n",
      "Training loss per sentence at iteration :  29  is  12.334126597973333 . Time taken :  225.87233471870422\n",
      "Training loss per sentence at iteration :  29  is  13.39235162103176 . Time taken :  11.124995231628418\n",
      "Training loss per sentence at iteration :  30  is  12.15261225469783 . Time taken :  225.68027186393738\n",
      "Training loss per sentence at iteration :  30  is  13.47633926641941 . Time taken :  11.078040361404419\n",
      "Training loss per sentence at iteration :  31  is  11.981284491367639 . Time taken :  227.09264302253723\n",
      "Training loss per sentence at iteration :  31  is  13.533534455299378 . Time taken :  11.031251430511475\n",
      "Training loss per sentence at iteration :  32  is  11.83154550104104 . Time taken :  225.99949860572815\n",
      "Training loss per sentence at iteration :  32  is  13.30074206495285 . Time taken :  11.062537431716919\n",
      "Training loss per sentence at iteration :  33  is  11.650424338211865 . Time taken :  234.23871970176697\n",
      "Training loss per sentence at iteration :  33  is  13.131637861222028 . Time taken :  10.999974727630615\n",
      "Training loss per sentence at iteration :  34  is  11.451455918023735 . Time taken :  226.3120801448822\n",
      "Training loss per sentence at iteration :  34  is  12.818096903562546 . Time taken :  11.34373164176941\n",
      "Training loss per sentence at iteration :  35  is  11.257146404788642 . Time taken :  228.84341192245483\n",
      "Training loss per sentence at iteration :  35  is  12.960627356648445 . Time taken :  11.171820878982544\n",
      "Training loss per sentence at iteration :  36  is  11.126421706543583 . Time taken :  225.4527087211609\n",
      "Training loss per sentence at iteration :  36  is  12.519861395902932 . Time taken :  11.03123140335083\n",
      "Training loss per sentence at iteration :  37  is  11.05455240353942 . Time taken :  227.06028509140015\n",
      "Training loss per sentence at iteration :  37  is  12.619781564593316 . Time taken :  11.109302759170532\n",
      "Training loss per sentence at iteration :  38  is  10.916050951606035 . Time taken :  241.43950986862183\n",
      "Training loss per sentence at iteration :  38  is  12.330204580247402 . Time taken :  11.531221628189087\n",
      "Training loss per sentence at iteration :  39  is  10.769306074669958 . Time taken :  242.4610631465912\n",
      "Training loss per sentence at iteration :  39  is  12.115053251385689 . Time taken :  11.093729734420776\n",
      "Training loss per sentence at iteration :  40  is  10.64758203760311 . Time taken :  226.65758895874023\n",
      "Training loss per sentence at iteration :  40  is  12.448664708554745 . Time taken :  11.140660047531128\n",
      "Training loss per sentence at iteration :  41  is  10.546645014730096 . Time taken :  227.8589324951172\n",
      "Training loss per sentence at iteration :  41  is  11.996544435143472 . Time taken :  11.156283378601074\n",
      "Training loss per sentence at iteration :  42  is  10.426571182664484 . Time taken :  227.10893845558167\n",
      "Training loss per sentence at iteration :  42  is  12.056175372242928 . Time taken :  11.109356880187988\n",
      "Training loss per sentence at iteration :  43  is  10.363711548032612 . Time taken :  419.4338467121124\n",
      "Training loss per sentence at iteration :  43  is  12.004396726816893 . Time taken :  22.951494455337524\n",
      "Training loss per sentence at iteration :  44  is  10.246959618060291 . Time taken :  443.2942714691162\n",
      "Training loss per sentence at iteration :  44  is  11.87625747269392 . Time taken :  11.156275033950806\n",
      "Training loss per sentence at iteration :  45  is  10.164601062637567 . Time taken :  232.097660779953\n",
      "Training loss per sentence at iteration :  45  is  11.781416893422604 . Time taken :  11.203049421310425\n",
      "Training loss per sentence at iteration :  46  is  10.020007950147987 . Time taken :  228.42262411117554\n",
      "Training loss per sentence at iteration :  46  is  11.597535253286361 . Time taken :  11.562511205673218\n",
      "Training loss per sentence at iteration :  47  is  9.976711449185014 . Time taken :  227.82588028907776\n",
      "Training loss per sentence at iteration :  47  is  11.465862308442592 . Time taken :  11.21867823600769\n",
      "Training loss per sentence at iteration :  48  is  9.855326939819752 . Time taken :  240.19194960594177\n",
      "Training loss per sentence at iteration :  48  is  11.282837003231048 . Time taken :  11.203101634979248\n",
      "Training loss per sentence at iteration :  49  is  9.7272721280545 . Time taken :  231.20747780799866\n",
      "Training loss per sentence at iteration :  49  is  11.420267143845559 . Time taken :  11.171903371810913\n",
      "Training loss per sentence at iteration :  50  is  9.657125590676815 . Time taken :  229.29644918441772\n",
      "Training loss per sentence at iteration :  50  is  11.342249171495437 . Time taken :  11.234405755996704\n",
      "Training loss per sentence at iteration :  51  is  9.513055131968855 . Time taken :  228.1332893371582\n",
      "Training loss per sentence at iteration :  51  is  11.138908515155315 . Time taken :  11.17186164855957\n",
      "Training loss per sentence at iteration :  52  is  9.504496337058395 . Time taken :  228.35898303985596\n",
      "Training loss per sentence at iteration :  52  is  11.295843735814094 . Time taken :  11.296852827072144\n",
      "Training loss per sentence at iteration :  53  is  9.509613976086676 . Time taken :  231.30661821365356\n",
      "Training loss per sentence at iteration :  53  is  11.203496801733971 . Time taken :  11.21867823600769\n",
      "Training loss per sentence at iteration :  54  is  9.427019555310533 . Time taken :  230.20278310775757\n",
      "Training loss per sentence at iteration :  54  is  10.994458924055099 . Time taken :  11.23435640335083\n",
      "Training loss per sentence at iteration :  55  is  9.35533244023323 . Time taken :  229.883540391922\n",
      "Training loss per sentence at iteration :  55  is  10.949486308336258 . Time taken :  11.203111171722412\n",
      "Training loss per sentence at iteration :  56  is  9.334024699425697 . Time taken :  229.73184633255005\n",
      "Training loss per sentence at iteration :  56  is  10.985900785207749 . Time taken :  11.28663182258606\n",
      "Training loss per sentence at iteration :  57  is  9.259184067014605 . Time taken :  232.842346906662\n",
      "Training loss per sentence at iteration :  57  is  10.749266713142395 . Time taken :  11.296802520751953\n",
      "Training loss per sentence at iteration :  58  is  9.177182772859187 . Time taken :  230.4681224822998\n",
      "Training loss per sentence at iteration :  58  is  11.104407945632934 . Time taken :  11.343728303909302\n",
      "Training loss per sentence at iteration :  59  is  9.146186487993598 . Time taken :  229.8083689212799\n",
      "Training loss per sentence at iteration :  59  is  10.72423036557436 . Time taken :  11.437479734420776\n",
      "Training loss per sentence at iteration :  60  is  9.11176976044327 . Time taken :  229.45274567604065\n",
      "Training loss per sentence at iteration :  60  is  10.922431462287904 . Time taken :  11.531227827072144\n",
      "Training loss per sentence at iteration :  61  is  9.04026248061061 . Time taken :  242.091961145401\n",
      "Training loss per sentence at iteration :  61  is  10.807382444620133 . Time taken :  11.656233310699463\n",
      "Training loss per sentence at iteration :  62  is  8.978243259984255 . Time taken :  244.31052827835083\n",
      "Training loss per sentence at iteration :  62  is  10.695665759563447 . Time taken :  11.328150510787964\n",
      "Training loss per sentence at iteration :  63  is  8.927247511015832 . Time taken :  229.82495164871216\n",
      "Training loss per sentence at iteration :  63  is  10.673655653357505 . Time taken :  11.34375548362732\n",
      "Training loss per sentence at iteration :  64  is  8.894933275800943 . Time taken :  230.51579928398132\n",
      "Training loss per sentence at iteration :  64  is  10.761298740684985 . Time taken :  11.39064073562622\n",
      "Training loss per sentence at iteration :  65  is  8.865561164444685 . Time taken :  233.6408040523529\n",
      "Training loss per sentence at iteration :  65  is  10.775749084174633 . Time taken :  11.34376072883606\n",
      "Training loss per sentence at iteration :  66  is  8.824546243113279 . Time taken :  231.1048972606659\n",
      "Training loss per sentence at iteration :  66  is  10.774550612926483 . Time taken :  11.296879291534424\n",
      "Training loss per sentence at iteration :  67  is  8.859453413541615 . Time taken :  230.45321035385132\n",
      "Training loss per sentence at iteration :  67  is  10.765188640117644 . Time taken :  11.406214714050293\n",
      "Training loss per sentence at iteration :  68  is  8.778814956597984 . Time taken :  231.94660329818726\n",
      "Training loss per sentence at iteration :  68  is  10.583996750831604 . Time taken :  11.531270742416382\n",
      "Training loss per sentence at iteration :  69  is  8.730074909600615 . Time taken :  235.35935401916504\n",
      "Training loss per sentence at iteration :  69  is  10.757049050569535 . Time taken :  11.343745470046997\n",
      "Training loss per sentence at iteration :  70  is  8.711815522036702 . Time taken :  231.2324924468994\n",
      "Training loss per sentence at iteration :  70  is  10.264228316783905 . Time taken :  11.411317348480225\n",
      "Training loss per sentence at iteration :  71  is  8.744959935748577 . Time taken :  231.64338445663452\n",
      "Training loss per sentence at iteration :  71  is  10.699243899822235 . Time taken :  11.328115701675415\n",
      "Training loss per sentence at iteration :  72  is  8.730802401569486 . Time taken :  231.52917623519897\n",
      "Training loss per sentence at iteration :  72  is  10.54853924882412 . Time taken :  11.343770503997803\n",
      "Training loss per sentence at iteration :  73  is  8.675769623850286 . Time taken :  235.4726858139038\n",
      "Training loss per sentence at iteration :  73  is  10.867605031967162 . Time taken :  11.296859979629517\n",
      "Training loss per sentence at iteration :  74  is  8.732846574623137 . Time taken :  231.63863348960876\n",
      "Training loss per sentence at iteration :  74  is  10.53354897737503 . Time taken :  11.421822547912598\n",
      "Training loss per sentence at iteration :  75  is  8.672650370711088 . Time taken :  232.72298049926758\n",
      "Training loss per sentence at iteration :  75  is  10.384920047044755 . Time taken :  11.484416723251343\n",
      "Training loss per sentence at iteration :  76  is  8.664815631583332 . Time taken :  233.71849489212036\n",
      "Training loss per sentence at iteration :  76  is  10.583803573608398 . Time taken :  11.421913385391235\n",
      "Training loss per sentence at iteration :  77  is  8.723887641054391 . Time taken :  236.2413911819458\n",
      "Training loss per sentence at iteration :  77  is  10.57921711230278 . Time taken :  11.375003576278687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per sentence at iteration :  78  is  8.67041477765739 . Time taken :  233.38076257705688\n",
      "Training loss per sentence at iteration :  78  is  10.610354239225387 . Time taken :  11.468691349029541\n",
      "Training loss per sentence at iteration :  79  is  8.585863729365915 . Time taken :  232.92167377471924\n",
      "Training loss per sentence at iteration :  79  is  10.463948942124844 . Time taken :  11.390565872192383\n",
      "Training loss per sentence at iteration :  80  is  8.661557903084159 . Time taken :  234.4440860748291\n",
      "Training loss per sentence at iteration :  80  is  10.319949804782867 . Time taken :  13.01561689376831\n",
      "Training loss per sentence at iteration :  81  is  8.604815092778951 . Time taken :  234.6403419971466\n",
      "Training loss per sentence at iteration :  81  is  10.540611353874207 . Time taken :  11.46874713897705\n",
      "Training loss per sentence at iteration :  82  is  8.549598021784425 . Time taken :  232.95106434822083\n",
      "Training loss per sentence at iteration :  82  is  10.315318251132965 . Time taken :  11.390605211257935\n",
      "Training loss per sentence at iteration :  83  is  8.591488912782072 . Time taken :  234.19694662094116\n",
      "Training loss per sentence at iteration :  83  is  10.198572796106339 . Time taken :  11.85930061340332\n",
      "Training loss per sentence at iteration :  84  is  8.557934339981898 . Time taken :  237.31219506263733\n",
      "Training loss per sentence at iteration :  84  is  10.534104591846466 . Time taken :  11.499990463256836\n",
      "Training loss per sentence at iteration :  85  is  8.528395236231386 . Time taken :  240.49610233306885\n",
      "Training loss per sentence at iteration :  85  is  10.355517524957657 . Time taken :  11.421885967254639\n",
      "Training loss per sentence at iteration :  86  is  8.531030851141363 . Time taken :  234.44171619415283\n",
      "Training loss per sentence at iteration :  86  is  10.193354172945023 . Time taken :  11.437527894973755\n",
      "Training loss per sentence at iteration :  87  is  8.488948485729098 . Time taken :  234.74964499473572\n",
      "Training loss per sentence at iteration :  87  is  10.035504745483399 . Time taken :  11.515677452087402\n",
      "Training loss per sentence at iteration :  88  is  8.444844499188662 . Time taken :  241.1792311668396\n",
      "Training loss per sentence at iteration :  88  is  10.551199119091034 . Time taken :  11.515552043914795\n",
      "Training loss per sentence at iteration :  89  is  8.401932566967606 . Time taken :  234.3904480934143\n",
      "Training loss per sentence at iteration :  89  is  10.369931827545166 . Time taken :  11.546800374984741\n",
      "Training loss per sentence at iteration :  90  is  8.383081931605934 . Time taken :  235.13260650634766\n",
      "Training loss per sentence at iteration :  90  is  10.236468510627747 . Time taken :  11.453113555908203\n",
      "Training loss per sentence at iteration :  91  is  8.32106707875207 . Time taken :  235.17160296440125\n",
      "Training loss per sentence at iteration :  91  is  10.17373394703865 . Time taken :  11.500056505203247\n",
      "Training loss per sentence at iteration :  92  is  8.329346429935843 . Time taken :  238.57439136505127\n",
      "Training loss per sentence at iteration :  92  is  10.336200712442398 . Time taken :  11.48441481590271\n",
      "Training loss per sentence at iteration :  93  is  8.316475164011122 . Time taken :  235.5621371269226\n",
      "Training loss per sentence at iteration :  93  is  10.236656327724457 . Time taken :  11.46869158744812\n",
      "Training loss per sentence at iteration :  94  is  8.284391711261124 . Time taken :  236.1696629524231\n",
      "Training loss per sentence at iteration :  94  is  10.058052722454072 . Time taken :  11.531233310699463\n",
      "Training loss per sentence at iteration :  95  is  8.232224850454926 . Time taken :  236.53147339820862\n",
      "Training loss per sentence at iteration :  95  is  10.218371545791626 . Time taken :  11.546587944030762\n",
      "Training loss per sentence at iteration :  96  is  8.238223654820025 . Time taken :  237.94598507881165\n",
      "Training loss per sentence at iteration :  96  is  10.28002024936676 . Time taken :  11.515264511108398\n",
      "Training loss per sentence at iteration :  97  is  8.286993457770347 . Time taken :  235.03496527671814\n",
      "Training loss per sentence at iteration :  97  is  10.256937791347504 . Time taken :  11.609023809432983\n",
      "Training loss per sentence at iteration :  98  is  8.239073954677583 . Time taken :  235.93026494979858\n",
      "Training loss per sentence at iteration :  98  is  9.940217051029204 . Time taken :  12.030989646911621\n",
      "Training loss per sentence at iteration :  99  is  8.19399593860507 . Time taken :  236.55106925964355\n",
      "Training loss per sentence at iteration :  99  is  10.157009321928024 . Time taken :  11.546701669692993\n"
     ]
    }
   ],
   "source": [
    "# lets start the training\n",
    "print('started training.....')\n",
    "\n",
    "for i in range(100):\n",
    "    # shuffle the training examples\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    startTime = time.time()\n",
    "    trainLoss = devLoss = 0\n",
    "    \n",
    "    for sent in train:\n",
    "        loss = computeLoss(sent)\n",
    "        trainLoss += loss.value()\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print(\"Training loss per sentence at iteration : \",i,\" is \",trainLoss/len(train),\". Time taken : \",(-startTime+time.time()))\n",
    "    \n",
    "    startTime = time.time()\n",
    "    for sent in dev:\n",
    "        loss = computeLoss(sent)\n",
    "        devLoss += loss.value()\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print(\"Training loss per sentence at iteration : \",i,\" is \",devLoss/len(dev),\". Time taken : \",(-startTime+time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 8057, 1, 2049, 147, 41, 168, 183, 8]\n",
      "['is', 'in', 'this', 'city', '.', '.', 'don', '&apos;t', 'done', '?', '.', 'don', '&apos;t', 'it', 'was', 'his', 'opinion', 'from', 'his', '.', 'don', '&apos;t', 'you', '?', 'don', '&apos;t', 'it', '?', 'wasn', '&apos;t', 'know', 'how', 'to', 'do', '.', 'about', 'the', 'not', '?', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', 'use', 'his', 'opinion', '.', '&quot;', 'no', 'use', 'the', 'city', '.', '&quot;', 'no', '.', 'we', 'can', 'off', 'his', 'paper', '.', 'too', 'the', 'crops', '.', '&quot;', 'goes', 'to', 'use', 'one', 'of', 'the', '.', 'opinion', '.', 'i', 'am', 'no', '.', 'i', 'can', '&apos;t', 'have', 'a', 'hat', '.', '&quot;', 'never', 'can', 'be', 'raining', '.', 'not', 'solve']\n",
      "[2527, 16, 1204, 5, 37, 10, 35, 515, 167, 8057, 60, 10, 35, 733, 6, 452, 69, 6, 42, 184, 41, 168, 183, 8]\n",
      "['i', 'have', 'to', 'have', 'follow', 'my', 'subject', '.', '&quot;', 'i', '&apos;ll', 'get', 'put', 'on', 'the', 'radio', '.', 'too', '.', 'it', 'has', 'saying', 'it', 'should', '.', '.', 'it', 'happened', '.', 'we', '&apos;ll', 'miss', 'the', 'doctor', '.', '&quot;', '&quot;', 'use', 'absolutely', 'gone', '.', 'i', '&apos;m', 'it', '.', '&quot;', '&quot;', 'i', '&apos;m', 'there', '.', 'don', '&apos;t', 'dead', 'eating', '.', 'you', 'could', 'get', 'his', 'part', '.', '&quot;', 'no', 'matter', 'seems', 'to', 'go', 'there', '.', 'we', 'can', '&apos;t', 'ashamed', '.', '.', 'i', 'would', 'like', 'him', 'to', 'join', 'him', '.', '.', 'it', 'is', 'too', 'difficult', '.', 'i', 'am', 'him', 'to', 'everyone', '.', '.', 'your', 'train', 'was']\n",
      "[43, 1, 2136, 898, 10, 132, 149, 8]\n",
      "['is', 'a', 'tokyo', 'for', 'sister', '.', 'gray', 'year', '.', '.', 'don', '&apos;t', 'young', 'he', 'is', 'as', 'anybody', 'does', 'not', 'as', 'well', 'as', 'it', 'alone', '.', 'state', 'marry', '@-@', '.', 'cheers', 'but', 'classical', 'a', '<unk>', '.', 'let', '&apos;s', 'london', '.', '&quot;', 'is', 'there', 'a', 'gold', 'informed', '.', 'his', 'son', 'is', '.', '&quot;', 'be', 'there', '.', 'no', 'one', 'one', 'for', 'the', 'accident', '.', '.', 'don', '&apos;t', 'let', '&apos;s', 'take', 'too', 'long', '.', 'a', 'car', 'fell', '.', 'not', 'ought', 'to', 'the', 'or', 'not', 'at', 'drink', '.', 'too', '.', 'it', 'is', 'out', 'of', 'the', 'problem', '.', 'he', 'is', 'or', 'else', '.', '.', 'where', 'can']\n",
      "[323, 1, 8057, 8057, 10, 4503, 35, 5170, 167, 69, 6, 5, 30, 8]\n",
      "['the', 'lot', 'of', 'fun', 'has', 'everything', 'the', 'students', 'road', '&apos;s', '<unk>', '.', '.', 'fuji', 'the', 'dinner', 'is', 'him', '.', 'children', '.', 'sudden', 'my', 'life', '.', 'life', 'will', 'christmas', '.', 'life', 'his', 'own', '.', 'experience', '.', 'we', 'are', 'too', 'abroad', '.', 'we', 'have', 'left', '.', 'our', 'lesson', 'them', 'for', '<unk>', '.', '.', 'it', 'looks', 'young', '.', 'because', '&apos;s', 'his', 'own', 'life', '.', 'made', '&apos;t', 'still', '.', '&quot;', '&quot;', 'he', 'haven', '&apos;s', 'had', '.', '&quot;', '&quot;', 'he', 'enjoyed', '<unk>', '.', '.', 'it', 'looks', 'like', 'the', 'dirty', '.', '&quot;', 'we', 'could', 'run', 'up', 'into', 'the', 'life', '.', 'your', 'children', 'comes', 'to', 'learn', '.']\n",
      "[8057, 13, 1, 1511, 20, 361, 627, 6, 5, 41, 42, 33, 8]\n",
      "['is', 'ready', 'ready', 'to', 'in', 'the', 'office', '.', 'snow', '?', '?', 'no', '!', '&quot;', 'no', 'little', '.', 'it', '&apos;s', 'because', 'the', 'pencil', 'was', 'a', 'person', '.', 'opinion', 'is', 'liked', '.', 'it', '&apos;s', 'tom', '&apos;s', 'we', '.', 'don', '&apos;t', 'it', 'does', 'the', 'room', 'without', 'easy', '.', '&quot;', 'it', 'is', 'have', 'no', '.', 'there', 'is', 'no', 'man', 'to', 'speak', '.', 'use', 'a', 'happy', 'life', '.', 'too', '.', 'his', 'we', 'must', '.', 'we', 'must', 'do', '.', 'we', 'must', 'the', 'opinion', '.', 'you', '&apos;t', 'like', 'the', '.', 'it', 'can', '&apos;t', 'it', 'much', 'of', 'the', '.', '&quot;', 'don', '&apos;t', 'it', 'too', 'to', 'me', '.', '&quot;']\n",
      "[148, 1, 2789, 10, 336, 3, 5, 69, 37, 62, 149, 179, 8]\n",
      "[',', 'i', '&apos;ve', 'got', 'ready', 'cold', '.', 'i', 'want', 'to', 'make', 'a', 'cold', '.', 'please', 'box', '&apos;t', '.', '&quot;', 'means', 'instead', 'of', ',', 'they', 'don', '&apos;t', 'want', 'any', 'noise', '.', '&quot;', '&quot;', 'turn', 'with', 'the', 'law', '.', '&quot;', 'says', 'does', 'the', 'he', '?', '&quot;', '&quot;', 'no', '.', 'there', 'is', 'no', 'am', 'so', 'much', 'of', 'her', 'opinion', '.', 'we', 'can', '&apos;t', 'our', 'room', '.', 'tom', 'was', 'your', 'failure', '.', 'you', 'could', 'be', 'gotten', '.', 'thanks', 'to', 'me', 'for', '.', 'you', 'were', 'miss', 'the', 'train', '.', 'why', 'his', 'life', 'is', 'my', 'sister', '.', 'let', '&apos;s', 'speak', 'so', 'so', 'noise', 'or', 'bill', '.']\n"
     ]
    }
   ],
   "source": [
    "# now lets translate few sentences\n",
    "sents = []\n",
    "sents.append(\"i like my steak medium\")\n",
    "sents.append(\"have you ever watched sumo wrestling ?\")\n",
    "\n",
    "sents_i = [[w2i_src[x] for x in sent.lower().strip().split(' ')] for sent in sents]\n",
    "\n",
    "i=0\n",
    "for sent,trans in dev:\n",
    "    print(sent)\n",
    "    out = translate(sent)\n",
    "    print(out)\n",
    "    i += 1\n",
    "    if(i>5):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
