{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------\n",
    "## Summary : Implementing the bag of words each word vector is of len 5\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time \n",
    "\n",
    "import random as random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# lets convert the words into integer\n",
    "# The default dictionary takes a function as input and outupts \n",
    "# it if key is not present in the map.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "\n",
    "# create an unknown token. As this is the first it will be 0\n",
    "UNK =w2i['<unk>']\n",
    "print(UNK)\n",
    "\n",
    "# Lets write a method to read the data.\n",
    "# This method returns the list of [features, label].\n",
    "# Here the features are integer ids of words and tags are labels\n",
    "def readDataSet(fileName):\n",
    "    retList = []\n",
    "    with open(fileName,'r+') as f:\n",
    "        for line in f:\n",
    "            tag,words = line.lower().strip().split(' ||| ')\n",
    "            \n",
    "            # now get the features which is the integerIds of words\n",
    "            features = [w2i[x] for x in words.split(' ')]\n",
    "            label = t2i[tag]\n",
    "            \n",
    "            # add the data to the list\n",
    "            retList.append([features,label])\n",
    "    return retList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18648 : 5\n",
      "train[0] : \n",
      " [[1, 2, 3, 4, 5, 6, 1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 9, 17, 5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33], 0]\n",
      "test[0] : \n",
      " [[1795, 71, 16582, 448], 2]\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "train = readDataSet('../data/classes/train.txt')\n",
    "dev = readDataSet(\"../data/classes/test.txt\")\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)\n",
    "\n",
    "# lets freeze the dictionary\n",
    "w2i = defaultdict(lambda:UNK, len(w2i))\n",
    "\n",
    "print(nwords,':',ntags)\n",
    "print('train[0] : \\n',train[0])\n",
    "print('test[0] : \\n',dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets create the model and trainer\n",
    "# we are using the adam trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# Create the parameters. \n",
    "W_sm = model.add_lookup_parameters((nwords,ntags)) # weights\n",
    "b_sm = model.add_parameters(ntags) # biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets write a method to compute the weights once we know w and b\n",
    "def computeScores(words):\n",
    "    # renew the Computation graph\n",
    "    dy.renew_cg()\n",
    "    scores = dy.esum([dy.lookup(W_sm,x) for x in words])\n",
    "    biases = dy.parameter(b_sm)\n",
    "    return scores+biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration  0  the training loss is :  19887.35503485799  time taken :  0.4295015335083008\n",
      "At iteration  0  the test accuracy is :  26.42533936651584 %\n",
      "At iteration  1  the training loss is :  16970.68979987502  time taken :  0.40904736518859863\n",
      "At iteration  1  the test accuracy is :  27.601809954751133 %\n",
      "At iteration  2  the training loss is :  15475.103072404861  time taken :  0.47750353813171387\n",
      "At iteration  2  the test accuracy is :  28.3710407239819 %\n",
      "At iteration  3  the training loss is :  14389.277297459543  time taken :  0.5425007343292236\n",
      "At iteration  3  the test accuracy is :  29.321266968325794 %\n",
      "At iteration  4  the training loss is :  13516.224422775209  time taken :  0.41250061988830566\n",
      "At iteration  4  the test accuracy is :  29.86425339366516 %\n",
      "At iteration  5  the training loss is :  12786.055393196642  time taken :  0.4030015468597412\n",
      "At iteration  5  the test accuracy is :  30.81447963800905 %\n",
      "At iteration  6  the training loss is :  12146.173193916678  time taken :  0.39148664474487305\n",
      "At iteration  6  the test accuracy is :  30.72398190045249 %\n",
      "At iteration  7  the training loss is :  11597.358409374952  time taken :  0.397005558013916\n",
      "At iteration  7  the test accuracy is :  31.80995475113122 %\n",
      "At iteration  8  the training loss is :  11102.216841995716  time taken :  0.3800480365753174\n",
      "At iteration  8  the test accuracy is :  32.262443438914026 %\n",
      "At iteration  9  the training loss is :  10656.93408549577  time taken :  0.37204718589782715\n",
      "At iteration  9  the test accuracy is :  32.44343891402715 %\n",
      "At iteration  10  the training loss is :  10249.47297757119  time taken :  0.41050076484680176\n",
      "At iteration  10  the test accuracy is :  32.62443438914027 %\n",
      "At iteration  11  the training loss is :  9878.774286426604  time taken :  0.44000935554504395\n",
      "At iteration  11  the test accuracy is :  33.393665158371036 %\n",
      "At iteration  12  the training loss is :  9537.112028732896  time taken :  0.4680039882659912\n",
      "At iteration  12  the test accuracy is :  33.755656108597286 %\n",
      "At iteration  13  the training loss is :  9226.007010772824  time taken :  0.4425008296966553\n",
      "At iteration  13  the test accuracy is :  34.11764705882353 %\n",
      "At iteration  14  the training loss is :  8934.898996040225  time taken :  0.38900041580200195\n",
      "At iteration  14  the test accuracy is :  33.393665158371036 %\n",
      "At iteration  15  the training loss is :  8660.008101314306  time taken :  0.3919525146484375\n",
      "At iteration  15  the test accuracy is :  34.38914027149321 %\n",
      "At iteration  16  the training loss is :  8407.11199119687  time taken :  0.4004507064819336\n",
      "At iteration  16  the test accuracy is :  33.52941176470588 %\n",
      "At iteration  17  the training loss is :  8174.687972806394  time taken :  0.4015007019042969\n",
      "At iteration  17  the test accuracy is :  34.29864253393665 %\n",
      "At iteration  18  the training loss is :  7950.235887549818  time taken :  0.4160006046295166\n",
      "At iteration  18  the test accuracy is :  34.796380090497735 %\n",
      "At iteration  19  the training loss is :  7746.259415019304  time taken :  0.3810441493988037\n",
      "At iteration  19  the test accuracy is :  34.796380090497735 %\n",
      "At iteration  20  the training loss is :  7548.9315935745835  time taken :  0.3794987201690674\n",
      "At iteration  20  the test accuracy is :  34.977375565610856 %\n",
      "At iteration  21  the training loss is :  7366.755336791277  time taken :  0.3849499225616455\n",
      "At iteration  21  the test accuracy is :  34.977375565610856 %\n",
      "At iteration  22  the training loss is :  7187.469649106264  time taken :  0.3874993324279785\n",
      "At iteration  22  the test accuracy is :  34.705882352941174 %\n",
      "At iteration  23  the training loss is :  7022.982693880796  time taken :  0.385495662689209\n",
      "At iteration  23  the test accuracy is :  34.660633484162894 %\n",
      "At iteration  24  the training loss is :  6870.460574641824  time taken :  0.3815000057220459\n",
      "At iteration  24  the test accuracy is :  35.1131221719457 %\n"
     ]
    }
   ],
   "source": [
    "# Now lets perform the the training\n",
    "for i in range(25):\n",
    "    # Perform the shuffling of the training data\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    # initialize the training loss and time\n",
    "    startTime = time.time()\n",
    "    trainLoss = 0\n",
    "    \n",
    "    # train\n",
    "    for words,tag in train:\n",
    "        loss = dy.pickneglogsoftmax(computeScores(words=words),tag)\n",
    "        trainLoss = trainLoss+loss.value()\n",
    "        \n",
    "        # compute gradients and update the parameters\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print('At iteration ',i,' the training loss is : ',trainLoss,' time taken : ',(time.time()-startTime))\n",
    "    \n",
    "    # test\n",
    "    testCorrect = 0\n",
    "    for words,tag in dev:\n",
    "        scores = computeScores(words=words).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        if(predict == tag):\n",
    "            testCorrect = testCorrect + 1\n",
    "    print('At iteration ',i,' the test accuracy is : ',(testCorrect/len(dev))*100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
