{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------\n",
    "## Summary : Implementing optimum version of NN language model\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the words to integer\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "\n",
    "# create the end of sentence and UNK tokens\n",
    "S = w2i['<s>']\n",
    "UNK = w2i['<unk>']\n",
    "\n",
    "# lets declare a method to read the data\n",
    "def readDataset(fileName):\n",
    "    retList = []\n",
    "    with open(fileName,'r+') as file:\n",
    "        for line in file:\n",
    "            words = [w2i[x] for x in line.lower().strip().split(' ')]\n",
    "            retList.append(words)\n",
    "            \n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary :  10000\n"
     ]
    }
   ],
   "source": [
    "# now lets read the data\n",
    "train = readDataset('../data/ptb/train.txt')\n",
    "test = readDataset('../data/ptb/valid.txt')\n",
    "\n",
    "# compute the number of words in the vocabulary\n",
    "nWords = len(w2i)\n",
    "print('Number of words in vocabulary : ',nWords)\n",
    "\n",
    "# lets freeze the dictionary\n",
    "w2i = defaultdict(lambda:UNK, len(w2i))\n",
    "\n",
    "# lets write a method to convert int to words\n",
    "i2w = {v:k for k,v in w2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets declare the embedding size and hidden layer size\n",
    "nEmb = 32\n",
    "nHid = 32\n",
    "\n",
    "# lets declare the model and the trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# let's declare the parameters\n",
    "W_emb = model.add_lookup_parameters((nWords,nEmb))\n",
    "\n",
    "# let's declare the rnn\n",
    "rnn = dy.LSTMBuilder(1,nEmb,nHid,model)\n",
    "\n",
    "# let's declare the softmax weights\n",
    "W_sm = model.add_parameters((nWords,nHid))\n",
    "b_sm = model.add_parameters((nWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(words):\n",
    "    # renew the computation graph\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    embeddings = [W_emb[x] for x in words]\n",
    "    \n",
    "    \n",
    "    # get the softmax weights\n",
    "    weightsSoftmax = dy.parameter(W_sm)\n",
    "    biasesSoftmax = dy.parameter(b_sm)\n",
    "    \n",
    "    # get the initial state of the rnn\n",
    "    rnnInit = rnn.initial_state()\n",
    "    \n",
    "    # add end of sentence to the rnn\n",
    "    state = rnnInit.add_input(W_emb[S])\n",
    "    \n",
    "    allLosses = []\n",
    "    for i,word in enumerate(words):\n",
    "        scores = weightsSoftmax*state.output() + biasesSoftmax\n",
    "        loss = dy.pickneglogsoftmax(scores,word)\n",
    "        allLosses.append(loss)\n",
    "        \n",
    "        # get the wordId for word and send it to rnn\n",
    "        emb = embeddings[i]\n",
    "        state = state.add_input(emb)\n",
    "        \n",
    "    return dy.esum(allLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting the training ...\n",
      "Iteration  0  :   TrainingLoss :  5392100.542980194  Number of words processed :  887521  Time taken :  601.6644756793976\n",
      "Iteration  0  :   TestLoss :  406730.8803231716\n",
      "Iteration  1  :   TrainingLoss :  4979143.804537058  Number of words processed :  887521  Time taken :  603.7007832527161\n",
      "Iteration  1  :   TestLoss :  396416.016207695\n",
      "Iteration  2  :   TrainingLoss :  4848180.005659819  Number of words processed :  887521  Time taken :  592.7751975059509\n",
      "Iteration  2  :   TestLoss :  392490.11943364143\n",
      "Iteration  3  :   TrainingLoss :  4785395.876128197  Number of words processed :  887521  Time taken :  909.9339401721954\n",
      "Iteration  3  :   TestLoss :  390762.5452759266\n",
      "Iteration  4  :   TrainingLoss :  4744179.611130238  Number of words processed :  887521  Time taken :  812.5940728187561\n",
      "Iteration  4  :   TestLoss :  388961.98898887634\n",
      "Iteration  5  :   TrainingLoss :  4712511.967818499  Number of words processed :  887521  Time taken :  836.5625920295715\n",
      "Iteration  5  :   TestLoss :  387575.67199087143\n",
      "Iteration  6  :   TrainingLoss :  4680155.963985205  Number of words processed :  887521  Time taken :  844.0455844402313\n",
      "Iteration  6  :   TestLoss :  386510.8398220539\n",
      "Iteration  7  :   TrainingLoss :  4655593.139440775  Number of words processed :  887521  Time taken :  1052.3537266254425\n",
      "Iteration  7  :   TestLoss :  386257.9978899956\n"
     ]
    }
   ],
   "source": [
    "# lets start the training process\n",
    "trainLosses = []\n",
    "testLosses = []\n",
    "\n",
    "bestTestLoss = 999999999\n",
    "lastTestLoss = 999999999\n",
    "\n",
    "print(' Starting the training ...')\n",
    "\n",
    "for i in range(10):\n",
    "    # randomly shuffle the training examples\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    # note start time and initialize the training loss and num of words processed\n",
    "    startTime = time.time()\n",
    "    trainLoss = 0.0\n",
    "    numOfWordsProcessed = 0\n",
    "    for sent in train:\n",
    "        # compute the loss\n",
    "        loss = computeLoss(sent)\n",
    "        trainLoss = trainLoss+loss.value()\n",
    "        numOfWordsProcessed = numOfWordsProcessed + len(sent)\n",
    "        \n",
    "        # compute the gradient and update the parameters\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    trainLosses.append(trainLoss)\n",
    "    print('Iteration ',i,' : ',' TrainingLoss : ',trainLoss,' Number of words processed : ',numOfWordsProcessed,' Time taken : ',\n",
    "         (time.time()-startTime))\n",
    "    \n",
    "    # evaluate on test data\n",
    "    testLoss = 0.0\n",
    "    for sent in test:\n",
    "        # compute loss\n",
    "        loss = computeLoss(sent)\n",
    "        testLoss = testLoss + loss.value()\n",
    "    testLosses.append(testLoss)\n",
    "    print('Iteration ',i,' : ',' TestLoss : ',testLoss)\n",
    "    \n",
    "    # check if current test loss is less than that of the previous testLoss if not reduce the learning rate by half\n",
    "    if(lastTestLoss<=testLoss):\n",
    "        trainer.learning_rate = trainer.learning_rate/2\n",
    "        \n",
    "    lastTestLoss = testLoss\n",
    "    \n",
    "    # check if current test loss is the best we have seen if so save the model\n",
    "    # disabling saving to save time\n",
    "    '''if(testLoss<bestTestLoss):\n",
    "        model.save(\"nn-optimum_LM.txt\")\n",
    "        bestTestLoss = testLoss'''\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
