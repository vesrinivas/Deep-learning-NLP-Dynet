{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------\n",
    "## Summary : Implementing optimum version of NN language model\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the words to integer\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "\n",
    "# create the end of sentence and UNK tokens\n",
    "S = w2i['<s>']\n",
    "UNK = w2i['<unk>']\n",
    "\n",
    "# lets declare a method to read the data\n",
    "def readDataset(fileName):\n",
    "    retList = []\n",
    "    with open(fileName,'r+') as file:\n",
    "        for line in file:\n",
    "            words = [w2i[x] for x in line.lower().strip().split(' ')]\n",
    "            retList.append(words)\n",
    "            \n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary :  10000\n"
     ]
    }
   ],
   "source": [
    "# now lets read the data\n",
    "train = readDataset('../data/ptb/train.txt')\n",
    "test = readDataset('../data/ptb/valid.txt')\n",
    "\n",
    "# compute the number of words in the vocabulary\n",
    "nWords = len(w2i)\n",
    "print('Number of words in vocabulary : ',nWords)\n",
    "\n",
    "# lets freeze the dictionary\n",
    "w2i = defaultdict(lambda:UNK, len(w2i))\n",
    "\n",
    "# lets write a method to convert int to words\n",
    "i2w = {v:k for k,v in w2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets declare the embedding size and hidden layer size\n",
    "nEmb = 32\n",
    "nHid = 32\n",
    "\n",
    "# lets declare the model and the trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# let's declare the parameters\n",
    "W_emb = model.add_lookup_parameters((nWords,nEmb))\n",
    "\n",
    "# let's declare the rnn\n",
    "rnn = dy.LSTMBuilder(1,nEmb,nHid,model)\n",
    "\n",
    "# let's declare the softmax weights\n",
    "W_sm = model.add_parameters((nWords,nHid))\n",
    "b_sm = model.add_parameters((nWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(sents):\n",
    "    # renew the computation graph\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # lets get the softmax weights\n",
    "    weightsSoftmax = dy.parameter(W_sm)\n",
    "    biasesSoftmax = dy.parameter(b_sm)\n",
    "    \n",
    "    # initialize the rnn state\n",
    "    state = rnn.initial_state()\n",
    "    \n",
    "    # get the wids and masks for each step\n",
    "    totalWords = 0\n",
    "    wids = []\n",
    "    masks = []\n",
    "    \n",
    "    # always the longest sentence will be in begining. Get the ith word for each sentence\n",
    "    for i in range(len(sents[0])):\n",
    "        wids.append([(sent[i] if len(sent) > i else S) for sent in sents])\n",
    "        mask = [(1 if len(sent) > i else 0) for sent in sents]\n",
    "        masks.append(mask)\n",
    "        \n",
    "        totalWords += sum(mask)\n",
    "        \n",
    "    # start the rnn by inputting <s>\n",
    "    init_ids = [S] * len(sents)\n",
    "    state = state.add_input(dy.lookup_batch(W_emb,init_ids))\n",
    "    \n",
    "    allLosses = []\n",
    "    \n",
    "    #feed each word to rnn and predict the next word\n",
    "    for wid,mask in zip(wids,masks):\n",
    "        # compute the scores biasesSoftmax + weightsSoftmax*state.output()\n",
    "        scores = dy.affine_transform([biasesSoftmax,weightsSoftmax,state.output()])\n",
    "        loss = dy.pickneglogsoftmax_batch(scores,wid)\n",
    "        \n",
    "        # don't count the loss if the mask is not 1\n",
    "        if(mask[-1]!=1):\n",
    "            mask_expr = dy.inputVector(mask)\n",
    "            mask_expr = dy.reshape(mask_expr, (1,), len(sents))\n",
    "            loss = loss*mask_expr\n",
    "            \n",
    "        allLosses.append(loss)\n",
    "        \n",
    "        # update the state of rnn\n",
    "        state = state.add_input(dy.lookup_batch(W_emb,wid))\n",
    "        \n",
    "    return dy.sum_batches(dy.esum(allLosses)), totalWords\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0  :   TrainingLoss :  6.04545369317854  Number of words processed :  887521  Time taken :  298.27337551116943\n",
      "Iteration  0  :   TestingLoss :  5.912341651965278  Number of words processed :  70390\n",
      "Iteration  1  :   TrainingLoss :  5.780199519468154  Number of words processed :  887521  Time taken :  307.05250787734985\n",
      "Iteration  1  :   TestingLoss :  5.732103717758974  Number of words processed :  70390\n",
      "Iteration  2  :   TrainingLoss :  5.6164246383863485  Number of words processed :  887521  Time taken :  325.9535667896271\n",
      "Iteration  2  :   TestingLoss :  5.625986420666228  Number of words processed :  70390\n",
      "Iteration  3  :   TrainingLoss :  5.500422291772433  Number of words processed :  887521  Time taken :  312.1986541748047\n",
      "Iteration  3  :   TestingLoss :  5.552818499398614  Number of words processed :  70390\n",
      "Iteration  4  :   TrainingLoss :  5.408813760287597  Number of words processed :  887521  Time taken :  265.47605180740356\n",
      "Iteration  4  :   TestingLoss :  5.497830079776826  Number of words processed :  70390\n",
      "Iteration  5  :   TrainingLoss :  5.333754132623763  Number of words processed :  887521  Time taken :  267.253466129303\n",
      "Iteration  5"
     ]
    }
   ],
   "source": [
    "# lets define the minibatch size\n",
    "MB_SIZE = 16\n",
    "\n",
    "# now we need to sort the train and test sentences in descending order\n",
    "train.sort(key=lambda x:-len(x))\n",
    "test.sort(key=lambda x:-len(x))\n",
    "\n",
    "trainOrder = [x * MB_SIZE for x in range(int((len(train) - 1) / MB_SIZE + 1))]\n",
    "testOrder = [x * MB_SIZE for x in range(int((len(test) - 1)/ MB_SIZE + 1))]\n",
    "\n",
    "\n",
    "# let's start the training\n",
    "for i in range(10):\n",
    "    # shuffle the training data\n",
    "    random.shuffle(trainOrder)\n",
    "    startTime = time.time()\n",
    "    \n",
    "    trainLoss = 0\n",
    "    totalWords = 0\n",
    "    for sid in trainOrder:\n",
    "        loss_exp, mb_words = computeLoss(train[sid:sid + MB_SIZE])\n",
    "        \n",
    "        trainLoss += loss_exp.scalar_value()\n",
    "        totalWords += mb_words\n",
    "        loss_exp.backward()\n",
    "        trainer.update()\n",
    "        \n",
    "    print('Iteration ',i,' : ',' TrainingLoss : ',trainLoss/totalWords,' Number of words processed : ',totalWords,' Time taken : ',\n",
    "         (time.time()-startTime))\n",
    "    trainer.update_epoch(1.0)\n",
    "    \n",
    "    testLoss = 0\n",
    "    testTotalWords = 0\n",
    "    for sid in testOrder:\n",
    "        loss_exp, mb_words = computeLoss(test[sid:sid + MB_SIZE])\n",
    "        \n",
    "        testLoss += loss_exp.scalar_value()\n",
    "        testTotalWords += mb_words\n",
    "        \n",
    "    print('Iteration ',i,' : ',' TestingLoss : ',testLoss/testTotalWords,' Number of words processed : ',testTotalWords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
