{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------------\n",
    "## Summary : use LSTMS to retrieve english text from japanese query \n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets convert word to integer. Since source and target languages are different\n",
    "# we use two different converters\n",
    "w2i_src = defaultdict(lambda: len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
    "\n",
    "# insert UNK into both languages\n",
    "UNK_SRC = w2i_src['<unk>']\n",
    "UNK_TRG = w2i_trg['<unk>']\n",
    "\n",
    "# lets write a method to read the sentences from two languages at the same time\n",
    "def readDataset(srcFileName,trgFileName):\n",
    "    retList = []\n",
    "    \n",
    "    with open(srcFileName,'r+',encoding='utf8') as src, open(trgFileName,'r+',encoding='utf8') as trg:\n",
    "        for srcLine,trgLine in zip(src,trg):\n",
    "            srcWords = [w2i_src[x] for x in srcLine.lower().strip().split()]\n",
    "            trgWords = [w2i_trg[x] for x in trgLine.strip().split()]\n",
    "            retList.append([srcWords,trgWords])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of japanese words :  8251\n",
      "number of english words :  7212\n"
     ]
    }
   ],
   "source": [
    "# read the train and test dataSets\n",
    "train = readDataset('../data/parallel/train.ja','../data/parallel/train.en')\n",
    "test = readDataset('../data/parallel/dev.ja','../data/parallel/dev.en')\n",
    "\n",
    "nWords_src = len(w2i_src)\n",
    "nWords_trg = len(w2i_trg)\n",
    "\n",
    "# lets freeze the dictionary\n",
    "w2i_src = defaultdict(lambda:UNK, len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda:UNK, len(w2i_trg))\n",
    "\n",
    "print('number of japanese words : ',nWords_src)\n",
    "print('number of english words : ',nWords_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets declare the embedding,batchSize and hidden layer sizes\n",
    "nEmb = 64\n",
    "nHid = 128\n",
    "MB_SIZE = 16\n",
    "\n",
    "# lets declare the model and trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# lets declare the embeddings for src and target\n",
    "W_emb_src = model.add_lookup_parameters((nWords_src,nEmb))\n",
    "W_emb_trg = model.add_lookup_parameters((nWords_trg,nEmb))\n",
    "\n",
    "# lets declare the lSTMS for the model.we will be using 2 for src and 2 for trg\n",
    "fwdLSTM_src = dy.LSTMBuilder(1,nEmb,nHid/2,model)\n",
    "bwdLSTM_src = dy.LSTMBuilder(1,nEmb,nHid/2,model)\n",
    "fwdLSTM_trg = dy.LSTMBuilder(1,nEmb,nHid/2,model)\n",
    "bwdLSTM_trg = dy.LSTMBuilder(1,nEmb,nHid/2,model)\n",
    "\n",
    "# we don't need softmax layer as we will be directly comparing the hidden layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to encode the sentences\n",
    "def encodeSent(weightsEMB,fwd,bwd,sents):\n",
    "    embs = [[weightsEMB[x] for x in sent] for sent in sents]\n",
    "    return [dy.concatenate([fwd.transduce(x)[-1], bwd.transduce(x)[-1]]) for x in embs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(sents):\n",
    "    # renew the computation graph\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    # get the initial state of the rnns\n",
    "    fwd_src = fwdLSTM_src.initial_state()\n",
    "    bwd_src = bwdLSTM_src.initial_state()\n",
    "    fwd_trg = fwdLSTM_trg.initial_state()\n",
    "    bwd_trg = bwdLSTM_trg.initial_state()\n",
    "    \n",
    "    # encode the sentences\n",
    "    source_rep = encodeSent(W_emb_src,fwd_src,bwd_src,[src for src,trg in sents])\n",
    "    target_rep = encodeSent(W_emb_trg,fwd_trg,bwd_trg,[trg for src,trg in sents])\n",
    "    \n",
    "    # concatanate the columns to form a matrix\n",
    "    mat_src = dy.concatenate_cols(source_rep)\n",
    "    mat_trg = dy.concatenate_cols(target_rep)\n",
    "    \n",
    "    # multiply the src and target matrices to get the similarity\n",
    "    sim_mat = dy.transpose(mat_src)*mat_trg\n",
    "    \n",
    "    # compute the hinge loss over all the dimensions\n",
    "    hingeLoss = dy.hinge_dim(sim_mat,list(range(len(sents))),d=1,m=0.1) # margin is taken as 0.1 and true labels are elements\n",
    "    # we can also compute the loss in both directions\n",
    "    # hingeLoss2 = dy.hinge_dim(sim_mat,list(range(len(sents))),d=0,m=0.1)\n",
    "    \n",
    "    return dy.sum_elems(hingeLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to index the corpus. This is later used to retrieve text\n",
    "def indexCorpus(sents):\n",
    "    retList = []\n",
    "    \n",
    "    # we are doing batching hence we are doing several sentences at the same time\n",
    "    for sid in range(0,len(sents),MB_SIZE):\n",
    "        # renew the computation graph\n",
    "        dy.renew_cg()\n",
    "        \n",
    "        # get the initial states of the rnns\n",
    "        fwd_src = fwdLSTM_src.initial_state()\n",
    "        bwd_src = bwdLSTM_src.initial_state()\n",
    "        fwd_trg = fwdLSTM_trg.initial_state()\n",
    "        bwd_trg = bwdLSTM_trg.initial_state()\n",
    "        \n",
    "        # encode the sentences in source and target\n",
    "        src_rep = encodeSent(W_emb_src,fwd_src,bwd_src,[src for src,trg in sents[sid:min(sid+MB_SIZE,len(sents))]])\n",
    "        trg_rep = encodeSent(W_emb_src,fwd_trg,bwd_trg,[trg for src,trg in sents[sid:min(sid+MB_SIZE,len(sents))]])\n",
    "        \n",
    "        for src_expr,trg_expr in zip(src_rep,trg_rep):\n",
    "            retList.append([src_expr,trg_expr])\n",
    "    \n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform retrieval, and return both scores and ranked order of candidates\n",
    "def retrieve(src, db_mtx):\n",
    "    scores = np.dot(db_mtx,src)\n",
    "    ranks = np.argsort(-scores)\n",
    "    return ranks, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per sentence at iteration :  0  is  0.5724111133098603 . Time taken :  -87.35064840316772\n",
      "Training loss per sentence at iteration :  0  is  0.29522717463970183 . Time taken :  -1.3724799156188965\n",
      "Training loss per sentence at iteration :  1  is  0.2053221986114979 . Time taken :  -81.56462407112122\n",
      "Training loss per sentence at iteration :  1  is  0.21539960038661957 . Time taken :  -1.2559986114501953\n",
      "Training loss per sentence at iteration :  2  is  0.1016724161490798 . Time taken :  -79.67263054847717\n",
      "Training loss per sentence at iteration :  2  is  0.2190750108361244 . Time taken :  -1.2630300521850586\n",
      "Training loss per sentence at iteration :  3  is  0.05503851629793644 . Time taken :  -77.73596835136414\n",
      "Training loss per sentence at iteration :  3  is  0.2443950602710247 . Time taken :  -1.25600004196167\n",
      "Training loss per sentence at iteration :  4  is  0.031489483015984296 . Time taken :  -79.1491470336914\n",
      "Training loss per sentence at iteration :  4  is  0.25724555051326753 . Time taken :  -1.2519984245300293\n",
      "Training loss per sentence at iteration :  5  is  0.02140843115951866 . Time taken :  -77.83380937576294\n",
      "Training loss per sentence at iteration :  5  is  0.25944374400377274 . Time taken :  -1.2659938335418701\n",
      "Training loss per sentence at iteration :  6  is  0.014700852040573954 . Time taken :  -77.88334393501282\n",
      "Training loss per sentence at iteration :  6  is  0.23729997032880784 . Time taken :  -1.2020001411437988\n",
      "Training loss per sentence at iteration :  7  is  0.011076968260109425 . Time taken :  -78.03240752220154\n",
      "Training loss per sentence at iteration :  7  is  0.2693741310238838 . Time taken :  -1.2897894382476807\n",
      "Training loss per sentence at iteration :  8  is  0.009166727498173714 . Time taken :  -79.04006218910217\n",
      "Training loss per sentence at iteration :  8  is  0.2610635238289833 . Time taken :  -1.289975881576538\n",
      "Training loss per sentence at iteration :  9  is  0.00655579678863287 . Time taken :  -78.97729706764221\n",
      "Training loss per sentence at iteration :  9  is  0.29695008635520936 . Time taken :  -1.2829995155334473\n",
      "Training loss per sentence at iteration :  10  is  0.005232244728412479 . Time taken :  -77.64819431304932\n",
      "Training loss per sentence at iteration :  10  is  0.2788662495613098 . Time taken :  -1.333134651184082\n",
      "Training loss per sentence at iteration :  11  is  0.004658754436671734 . Time taken :  -78.74791598320007\n",
      "Training loss per sentence at iteration :  11  is  0.31417656981945036 . Time taken :  -1.4610011577606201\n",
      "Training loss per sentence at iteration :  12  is  0.00562943482324481 . Time taken :  -79.38217568397522\n",
      "Training loss per sentence at iteration :  12  is  0.3288686744570732 . Time taken :  -1.2589998245239258\n",
      "Training loss per sentence at iteration :  13  is  0.004243676791340112 . Time taken :  -79.45970582962036\n",
      "Training loss per sentence at iteration :  13  is  0.33391155195236205 . Time taken :  -1.2682349681854248\n",
      "Training loss per sentence at iteration :  14  is  0.004951388756930828 . Time taken :  -78.09825873374939\n",
      "Training loss per sentence at iteration :  14  is  0.2727633092403412 . Time taken :  -1.3530447483062744\n",
      "Training loss per sentence at iteration :  15  is  0.004224456484615803 . Time taken :  -78.81198358535767\n",
      "Training loss per sentence at iteration :  15  is  0.3002070146203041 . Time taken :  -1.299999713897705\n",
      "Training loss per sentence at iteration :  16  is  0.0037020017825067043 . Time taken :  -77.1951253414154\n",
      "Training loss per sentence at iteration :  16  is  0.2846846150159836 . Time taken :  -1.2550132274627686\n",
      "Training loss per sentence at iteration :  17  is  0.0033276812069118022 . Time taken :  -84.02895188331604\n",
      "Training loss per sentence at iteration :  17  is  0.28073828279972074 . Time taken :  -1.3195607662200928\n",
      "Training loss per sentence at iteration :  18  is  0.0030846349343657494 . Time taken :  -86.82511949539185\n",
      "Training loss per sentence at iteration :  18  is  0.2877216416597366 . Time taken :  -1.4110407829284668\n",
      "Training loss per sentence at iteration :  19  is  0.0028589425578713417 . Time taken :  -84.54953384399414\n",
      "Training loss per sentence at iteration :  19  is  0.2820606851577759 . Time taken :  -1.3199687004089355\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to use a stale expression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6a70325d6c90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mrecall_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mranks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrg_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# check if i is first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-e520691a9226>\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(src, db_mtx)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Perform retrieval, and return both scores and ranked order of candidates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb_mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mranks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.Expression.__mul__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet._mul\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.ensure_freshness\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to use a stale expression."
     ]
    }
   ],
   "source": [
    "# lets start the training\n",
    "\n",
    "for i in range(20):\n",
    "    trainLoss = 0\n",
    "    startTime = time.time()\n",
    "    for sid in range(0,len(train),MB_SIZE):\n",
    "        # compute the loss\n",
    "        loss = computeLoss(train[sid:min(sid+MB_SIZE,len(train))])\n",
    "        trainLoss += loss.scalar_value()\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print(\"Training loss per sentence at iteration : \",i,\" is \",trainLoss/len(train),\". Time taken : \",(startTime-time.time()))\n",
    "    \n",
    "    testLoss = 0\n",
    "    startTime = time.time()\n",
    "    for sid in range(0,len(test),MB_SIZE):\n",
    "        # compute the loss\n",
    "        loss = computeLoss(test[sid:min(sid+MB_SIZE,len(test))])\n",
    "        testLoss += loss.scalar_value()\n",
    "    print(\"Training loss per sentence at iteration : \",i,\" is \",testLoss/len(test),\". Time taken : \",(startTime-time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to use a stale expression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e1c2278af504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrecall_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mranks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrg_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# check if i is first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-15ab52b53671>\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(src, db_mtx)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenew_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb_mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mranks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.Expression.__mul__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet._mul\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.ensure_freshness\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to use a stale expression."
     ]
    }
   ],
   "source": [
    "# evaluate the actual retrival\n",
    "reps = indexCorpus(test)\n",
    "trg_mat = np.stack(trg for src,trg in reps)\n",
    "    \n",
    "# initialize the losses\n",
    "recall_1 = recall_5 = recall_10 = 0\n",
    "for i,(src,trg) in enumerate(reps):\n",
    "    ranks,scores = retrieve(src,trg_mat)\n",
    "        \n",
    "    # check if i is first\n",
    "    if ranks[0] == i:\n",
    "        recall_1 += 1\n",
    "            \n",
    "    # check if i is in top 5    \n",
    "    if i in ranks[:5]:\n",
    "        recall_5 += 1\n",
    "        \n",
    "    # check if i is in top 10\n",
    "    if i in ranks[:10]:\n",
    "        recall_10 += 1\n",
    "            \n",
    "    print('Recall values : ',recall_1/len(test),' : ',recall_5/len(test),' : ',recall_10/len(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
