{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------------\n",
    "## Summary : Implementing simple RNN for sentiment classification\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets convert the words into integer\n",
    "# The default dictionary takes a function as input and outupts \n",
    "# it if key is not present in the map.\n",
    "w2i = defaultdict(lambda : len(w2i))\n",
    "t2i = defaultdict(lambda : len(t2i))\n",
    "\n",
    "# create an unknown token. As this is the first it will be 0\n",
    "UNK =w2i['<unk>']\n",
    "\n",
    "# Lets write a method to read the data.\n",
    "# This method returns the list of [features, label].\n",
    "# Here the features are integer ids of words and tags are labels\n",
    "def readDataSet(fileName):\n",
    "    retList = []\n",
    "    with open(fileName,'r+') as f:\n",
    "        for line in f:\n",
    "            tag,sentence = line.lower().strip().split(' ||| ') \n",
    "            words = sentence.lower().strip().split(' ')\n",
    "            features = [w2i[x] for x in words]\n",
    "            label = t2i[tag]\n",
    "            \n",
    "            retList.append([features,label])\n",
    "            \n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18648  :  5\n"
     ]
    }
   ],
   "source": [
    "train = readDataSet('../data/classes/train.txt')\n",
    "test = readDataSet('../data/classes/test.txt')\n",
    "nWords = len(w2i)\n",
    "ntags = len(t2i)\n",
    "\n",
    "# lets freeze the dictionary\n",
    "w2i = defaultdict(lambda:UNK, len(w2i))\n",
    "\n",
    "print(nWords,' : ',ntags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets define the embedding size and hidden layer size\n",
    "nEmb = 64\n",
    "nHid = 64\n",
    "\n",
    "# lets define the model and trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# lets define the model\n",
    "W_emb = model.add_lookup_parameters((nWords,nEmb))\n",
    "\n",
    "# lets add two rnns. one reads from start to end and other from end to start of sentence\n",
    "forwardRNN =  dy.SimpleRNNBuilder(1,nEmb,nHid,model)\n",
    "backWardRNN = dy.SimpleRNNBuilder(1,nEmb,nHid,model)\n",
    "\n",
    "# lets declare the softmax weights\n",
    "W_sm = model.add_parameters((ntags,2*nHid))\n",
    "b_sm = model.add_parameters((ntags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to compute the scores\n",
    "def computeScores(words):\n",
    "    # renew computation graph\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    embeddings = [W_emb[x] for x in words]\n",
    "    \n",
    "    # get the init state for the rnns\n",
    "    finit = forwardRNN.initial_state()\n",
    "    binit = backWardRNN.initial_state()\n",
    "    \n",
    "    # pass the embeddings throught the rnns and get the output\n",
    "    fwdEmbs = finit.transduce(embeddings)\n",
    "    embeddings.reverse()\n",
    "    bwdEmbs = binit.transduce(embeddings)\n",
    "    \n",
    "    # get the last ones in the fwd and bwd embeddings. this will represent the entire sentence state\n",
    "    # and concatanate them\n",
    "    finalEmb = dy.concatenate([fwdEmbs[-1],bwdEmbs[-1]])\n",
    "    \n",
    "    # get the weights of softmax layers\n",
    "    weightsSoftmax = dy.parameter(W_sm)\n",
    "    biasesSoftmax = dy.parameter(b_sm)\n",
    "    \n",
    "    \n",
    "    return weightsSoftmax*finalEmb + biasesSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration  0  the training accuracy is :  0.8730102996254682  time taken :  5.757597923278809\n",
      "At iteration  0  the test accuracy is :  0.3565610859728507  time taken :  6.317943811416626\n",
      "At iteration  1  the training accuracy is :  0.87312734082397  time taken :  5.539999485015869\n",
      "At iteration  1  the test accuracy is :  0.3565610859728507  time taken :  6.00378680229187\n",
      "At iteration  2  the training accuracy is :  0.8735955056179775  time taken :  5.263608932495117\n",
      "At iteration  2  the test accuracy is :  0.3565610859728507  time taken :  5.734323740005493\n",
      "At iteration  3  the training accuracy is :  0.8735955056179775  time taken :  5.313278913497925\n",
      "At iteration  3  the test accuracy is :  0.3565610859728507  time taken :  5.779788970947266\n",
      "At iteration  4  the training accuracy is :  0.8737125468164794  time taken :  5.1940436363220215\n",
      "At iteration  4  the test accuracy is :  0.3565610859728507  time taken :  5.66184139251709\n",
      "At iteration  5  the training accuracy is :  0.8735955056179775  time taken :  5.356337785720825\n",
      "At iteration  5  the test accuracy is :  0.3565610859728507  time taken :  5.818676471710205\n",
      "At iteration  6  the training accuracy is :  0.8737125468164794  time taken :  5.2734150886535645\n",
      "At iteration  6  the test accuracy is :  0.3565610859728507  time taken :  5.743683099746704\n",
      "At iteration  7  the training accuracy is :  0.8738295880149812  time taken :  5.875861644744873\n",
      "At iteration  7  the test accuracy is :  0.3561085972850679  time taken :  6.345900535583496\n",
      "At iteration  8  the training accuracy is :  0.8738295880149812  time taken :  5.60253381729126\n",
      "At iteration  8  the test accuracy is :  0.3561085972850679  time taken :  6.082418441772461\n",
      "At iteration  9  the training accuracy is :  0.8739466292134831  time taken :  7.598782777786255\n",
      "At iteration  9  the test accuracy is :  0.3561085972850679  time taken :  8.350892305374146\n",
      "At iteration  10  the training accuracy is :  0.8740636704119851  time taken :  9.000488042831421\n",
      "At iteration  10  the test accuracy is :  0.3561085972850679  time taken :  9.699986219406128\n",
      "At iteration  11  the training accuracy is :  0.8740636704119851  time taken :  6.955716371536255\n",
      "At iteration  11  the test accuracy is :  0.3561085972850679  time taken :  7.508134603500366\n",
      "At iteration  12  the training accuracy is :  0.8741807116104869  time taken :  6.4493138790130615\n",
      "At iteration  12  the test accuracy is :  0.3561085972850679  time taken :  6.929635286331177\n",
      "At iteration  13  the training accuracy is :  0.8742977528089888  time taken :  6.075646877288818\n",
      "At iteration  13  the test accuracy is :  0.3561085972850679  time taken :  6.529759168624878\n",
      "At iteration  14  the training accuracy is :  0.8741807116104869  time taken :  7.178334712982178\n",
      "At iteration  14  the test accuracy is :  0.3561085972850679  time taken :  7.780263423919678\n",
      "At iteration  15  the training accuracy is :  0.8744147940074907  time taken :  6.4347312450408936\n",
      "At iteration  15  the test accuracy is :  0.3561085972850679  time taken :  6.905428647994995\n",
      "At iteration  16  the training accuracy is :  0.8744147940074907  time taken :  5.360336780548096\n",
      "At iteration  16  the test accuracy is :  0.3561085972850679  time taken :  5.926692008972168\n",
      "At iteration  17  the training accuracy is :  0.8744147940074907  time taken :  5.67992377281189\n",
      "At iteration  17  the test accuracy is :  0.3561085972850679  time taken :  6.140138387680054\n",
      "At iteration  18  the training accuracy is :  0.8745318352059925  time taken :  5.109945058822632\n",
      "At iteration  18  the test accuracy is :  0.3561085972850679  time taken :  5.566182374954224\n",
      "At iteration  19  the training accuracy is :  0.8745318352059925  time taken :  5.204272270202637\n",
      "At iteration  19  the test accuracy is :  0.3561085972850679  time taken :  5.660302639007568\n",
      "At iteration  20  the training accuracy is :  0.8746488764044944  time taken :  5.339497327804565\n",
      "At iteration  20  the test accuracy is :  0.3561085972850679  time taken :  5.83291482925415\n",
      "At iteration  21  the training accuracy is :  0.8746488764044944  time taken :  5.264496326446533\n",
      "At iteration  21  the test accuracy is :  0.3561085972850679  time taken :  5.784326553344727\n",
      "At iteration  22  the training accuracy is :  0.8747659176029963  time taken :  5.28965950012207\n",
      "At iteration  22  the test accuracy is :  0.3561085972850679  time taken :  5.739878177642822\n",
      "At iteration  23  the training accuracy is :  0.8746488764044944  time taken :  5.31639289855957\n",
      "At iteration  23  the test accuracy is :  0.3561085972850679  time taken :  5.806254148483276\n",
      "At iteration  24  the training accuracy is :  0.8748829588014981  time taken :  5.287004709243774\n",
      "At iteration  24  the test accuracy is :  0.3556561085972851  time taken :  5.737146377563477\n"
     ]
    }
   ],
   "source": [
    "# Now lets perform the the training\n",
    "bestTestAccuracy = 0\n",
    "lastTestAccuracy = 0\n",
    "\n",
    "for i in range(25):\n",
    "    # Perform the shuffling of the training data\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    # initialize the training loss and time\n",
    "    startTime = time.time()\n",
    "    trainLoss = 0\n",
    "    trainCorrect = 0\n",
    "    \n",
    "    # train\n",
    "    for words,tag in train:\n",
    "        scores = computeScores(words)\n",
    "        predict = np.argmax(scores.npvalue())\n",
    "        if(predict==tag):\n",
    "            trainCorrect = trainCorrect+1\n",
    "        \n",
    "        loss = dy.pickneglogsoftmax(scores,tag)\n",
    "        trainLoss += loss.value()\n",
    "        \n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print('At iteration ',i,' the training accuracy is : ',(trainCorrect/len(train)),' time taken : ',(time.time()-startTime))\n",
    "    \n",
    "    testLoss = 0\n",
    "    testCorrect = 0\n",
    "    # test\n",
    "    for words,tag in test:\n",
    "        scores = computeScores(words)\n",
    "        predict = np.argmax(scores.npvalue())\n",
    "        if(predict==tag):\n",
    "            testCorrect += 1\n",
    "    testAccuracy = testCorrect/len(test)\n",
    "    if(testAccuracy>bestTestAccuracy):\n",
    "        bestTestAccuracy = testAccuracy\n",
    "    \n",
    "    if(testAccuracy<lastTestAccuracy):\n",
    "        trainer.learning_rate = trainer.learning_rate/2\n",
    "    lastTestAccuracy = testAccuracy\n",
    "    \n",
    "    print('At iteration ',i,' the test accuracy is : ',(testAccuracy),' time taken : ',(time.time()-startTime))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
