{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------------\n",
    "## Summary : perform pos tagging using bidirectional LSTM\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare the combination we want to use\n",
    "use_teacher_forcing = True\n",
    "use_structure_perceptron = True\n",
    "use_cost_augmented = True\n",
    "use_hinge = True\n",
    "use_schedule = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets convert the words into integer\n",
    "# The default dictionary takes a function as input and outupts \n",
    "# it if key is not present in the map.\n",
    "w2i = defaultdict(lambda : len(w2i))\n",
    "t2i = defaultdict(lambda : len(t2i))\n",
    "\n",
    "# lets write a method to read the data from file\n",
    "def readData(fileName):\n",
    "    retList = []\n",
    "    \n",
    "    with open(fileName,'r+',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            words = []\n",
    "            tags = []\n",
    "            for wt in line.strip().split():\n",
    "                word,tag = wt.split('|')\n",
    "                words.append(w2i[word])\n",
    "                tags.append(t2i[tag])\n",
    "            retList.append([words,tags])\n",
    "    return retList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29002  :  11\n"
     ]
    }
   ],
   "source": [
    "# read the train data \n",
    "train = readData('../data/tags/train.txt')\n",
    "unk_word = w2i[\"<unk>\"]\n",
    "w2i = defaultdict(lambda: unk_word, w2i)\n",
    "unk_tag = t2i[\"<unk>\"]\n",
    "start_tag = t2i[\"<start>\"]\n",
    "t2i = defaultdict(lambda: unk_tag, t2i)\n",
    "\n",
    "# read the test data\n",
    "test = readData('../data/tags/dev.txt')\n",
    "\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)\n",
    "print(nwords,' : ',ntags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a class for always true parser\n",
    "class AlwaysTrueSampler:\n",
    "    #An always true sampler, only sample from true distribution.\n",
    "\n",
    "    def sample_true(self):\n",
    "        return True\n",
    "\n",
    "    def decay(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# write a class for schedule sampler. This class initially gives\n",
    "# correct values and slowly starts giving wrong labels by using\n",
    "# simulated annealing\n",
    "class ScheduleSampler:\n",
    "    def __init__(self, start_rate=1, min_rate=0.2, decay_rate=0.1):\n",
    "        self.min_rate = min_rate\n",
    "        self.iter = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.start_rate = start_rate\n",
    "        self.reach_min = False\n",
    "        self.sample_rate = start_rate\n",
    "    \n",
    "    # method for modifying decay rate\n",
    "    def decay_func(self):\n",
    "        if not self.reach_min:\n",
    "            self.sample_rate = self.start_rate - self.iter * self.decay_rate\n",
    "            if self.sample_rate < self.min_rate:\n",
    "                self.reach_min = True\n",
    "                self.sample_rate = self.min_rate\n",
    "\n",
    "    def decay(self):\n",
    "        self.iter += 1\n",
    "        self.decay_func()\n",
    "        print(\"Sample rate is now \",self.sample_rate)\n",
    "        \n",
    "    def sample_true(self):\n",
    "        return (random.random() < self.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the appropriate sampler\n",
    "if use_schedule:\n",
    "    sampler = ScheduleSampler()\n",
    "else:\n",
    "    sampler = AlwaysTrueSampler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets declare the embedding size and hidden layer size\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "TAG_EMBED_SIZE = 16\n",
    "\n",
    "# lets declare the model and trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# lets declare the parameters to the model\n",
    "W_emb = model.add_lookup_parameters((nwords,EMB_SIZE))\n",
    "\n",
    "# use tag embeddings if teacher forcing is present\n",
    "if(use_teacher_forcing):\n",
    "    tag_emb = model.add_lookup_parameters((ntags,TAG_EMBED_SIZE))\n",
    "    \n",
    "# declare the forward lstm\n",
    "if(use_teacher_forcing):\n",
    "    fwdLstm = dy.LSTMBuilder(1,EMB_SIZE+TAG_EMBED_SIZE,HID_SIZE/2,model)\n",
    "else:\n",
    "    fwdLstm = dy.LSTMBuilder(1,EMB_SIZE,HID_SIZE/2,model)\n",
    "    \n",
    "# backward lstm is same in both cases\n",
    "bwdLstm = dy.LSTMBuilder(1,EMB_SIZE,HID_SIZE/2,model)\n",
    "\n",
    "# Word-level softmax\n",
    "W_sm = model.add_parameters((ntags, HID_SIZE))\n",
    "b_sm = model.add_parameters((ntags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to compute scores when teacher forcing is not present\n",
    "def computeScores(sent):\n",
    "    # renew cg\n",
    "    dy.renew_cg()\n",
    "    wSoftmax = dy.parameter(W_sm)\n",
    "    bSoftmax = dy.parameter(b_sm)\n",
    "        \n",
    "    # get the embeddings for the words\n",
    "    wordEmbs = [W_emb[x] for x in sent]\n",
    "    \n",
    "    # get the init state of fwdLSTM\n",
    "    finit = fwdLstm.initial_state()\n",
    "    fwd_word_reps = finit.transduce(wordEmbs)\n",
    "    \n",
    "    # get the init state of bwdLSTM\n",
    "    binit = bwdLstm.initial_state()\n",
    "    bwd_word_reps = binit.transduce(reversed(wordEmbs))\n",
    "    \n",
    "    # concatanate the two reps\n",
    "    wordReps = [dy.concatenate((fwd,bwd) for fwd,bwd in zip(fwd_word_reps,reversed(bwd_word_reps)))]\n",
    "    \n",
    "    # compute the scores\n",
    "    scores = [dy.affine_transform([bSoftmax,wSoftmax,wordRep]) for wordRep in wordReps]\n",
    "                \n",
    "    return scores\n",
    "\n",
    "\n",
    "# lets define a method to compute the scores when teacher forcing is present\n",
    "def computeScoresWithPrevTag(sent,tags=None):\n",
    "    # renew cg\n",
    "    dy.renew_cg()\n",
    "    wSoftmax = dy.parameter(W_sm)\n",
    "    bSoftmax = dy.parameter(b_sm)\n",
    "        \n",
    "    # get the embeddings for the words\n",
    "    wordEmbs = [W_emb[x] for x in sent]\n",
    "    \n",
    "    # get the init state of bwdLSTM\n",
    "    binit = bwdLstm.initial_state()\n",
    "    bwd_word_reps = binit.transduce(reversed(wordEmbs))\n",
    "    \n",
    "    # get the init state of fwdLSTM\n",
    "    finit = fwdLstm.initial_state()\n",
    "    \n",
    "    # initialize the previous tag to start tag\n",
    "    prevTag = start_tag\n",
    "    scores = []\n",
    "    \n",
    "    # iterate through the sentence and compute the scores\n",
    "    index = 0\n",
    "    for word,bwd_rep in zip(wordEmbs,reversed(bwd_word_reps)):\n",
    "        finit = finit.add_input(dy.concatenate([word,tag_emb[prevTag]]))\n",
    "        fullRep = dy.concatenate([finit.output(),bwd_rep])\n",
    "        \n",
    "        #perform softmax and compute score\n",
    "        score = dy.affine_transform([bSoftmax,wSoftmax,fullRep])\n",
    "        scores.append(score)\n",
    "        prediction = np.argmax(score.npvalue())\n",
    "        \n",
    "        # check if tags are given\n",
    "        if tags is not None:\n",
    "            # if sampler returns true give correct tag\n",
    "            if sampler.sample_true():\n",
    "                prevTag = tags[index]\n",
    "            # if sampler returns false give predicted tag \n",
    "            else :\n",
    "                prevTag = prediction\n",
    "            index += 1\n",
    "        else :\n",
    "            prevTag = prediction\n",
    "            \n",
    "    return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  now lets define various methods to compute losses,scores,correct Labels\n",
    "\n",
    "# compute MLE loss \n",
    "def computeMLELoss(scores, tags):\n",
    "    losses = [dy.pickneglogsoftmax(score, tag) for score, tag in zip(scores, tags)]\n",
    "    return dy.esum(losses)\n",
    "\n",
    "\n",
    "# compute the hamming loss\n",
    "def computeHammingLoss(predictions,actuals):\n",
    "    return sum([pred != actual for pred,actual in zip(predictions,actuals)])\n",
    "\n",
    "\n",
    "# compute the sequence scores. This is the score of entire sentence\n",
    "def computeSeqScores(scores, tags):\n",
    "    return dy.esum([score[int(tag)] for score, tag in zip(scores, tags)])\n",
    "\n",
    "\n",
    "# compute the hamming augmented decoder\n",
    "def hammingAugDecode(scores, tags):\n",
    "    augmentedResult = []\n",
    "    \n",
    "    for score,tag in zip(scores,tags):\n",
    "        originalScore = score.npvalue()\n",
    "        cost = np.ones(originalScore.shape)\n",
    "        # correct tag will have zero score added\n",
    "        cost[tag] = 0\n",
    "        augmentedResult.append(np.argmax(np.add(originalScore,cost)))\n",
    "    \n",
    "    return augmentedResult\n",
    "\n",
    "\n",
    "def computePercepLoss(scores, tags):\n",
    "    # compute the predictions\n",
    "    if use_cost_augmented:\n",
    "        predictions = hammingAugDecode(scores, tags)\n",
    "    else:\n",
    "        predictions = [np.argmax(score.npvalue()) for score in scores]\n",
    "\n",
    "    # declare the margin\n",
    "    margin = dy.scalarInput(-2)\n",
    "    \n",
    "    # check if predictions and tags are same\n",
    "    if predictions!=tags:\n",
    "        # compute the sequence scores\n",
    "        predScore = computeSeqScores(scores,predictions)\n",
    "        actualScore = computeSeqScores(scores,tags)\n",
    "        \n",
    "        # check if we want to use cost augmented loss\n",
    "        if use_cost_augmented:\n",
    "            hamming = dy.scalarInput(computeHammingLoss(predictions,tags))\n",
    "            loss = predScore + hamming - actualScore\n",
    "        else:\n",
    "            loss = predScore - actualScore\n",
    "        \n",
    "        # check if we want to use hinge loss\n",
    "        if use_hinge:\n",
    "            loss = dy.emax([dy.scalarInput(0),loss-margin])\n",
    "            \n",
    "    else :\n",
    "        loss = dy.scalarInput(0)\n",
    "    \n",
    "    return loss\n",
    "  \n",
    "# method to return loss    \n",
    "def computeLoss(scores,tags):\n",
    "    if use_structure_perceptron:\n",
    "        return computePercepLoss(scores,tags)\n",
    "    else:\n",
    "        return computeMLELoss(scores,tags)\n",
    "    \n",
    "    \n",
    "# compute the number of correct classifications\n",
    "def computeCorr(scores,tags):\n",
    "    correct = [(np.argmax(score.npvalue())==tag) for score,tag in zip(scores,tags)]\n",
    "    return sum(correct) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ....\n",
      "Train_loss at iter :  0  is  0.25094247251978047  Accuracy :  0.9208427740910667 . Time taken :  105.90236020088196\n",
      "Test_loss at iter :  0  is  0.37651000948397484  Accuracy :  0.8714076960545543 . Time taken :  7.030837535858154\n",
      "Train_loss at iter :  1  is  0.15612875653871944  Accuracy :  0.9614615502463332 . Time taken :  100.41570615768433\n",
      "Test_loss at iter :  1  is  0.376483876095111  Accuracy :  0.8838669982310867 . Time taken :  6.929711818695068\n",
      "Train_loss at iter :  2  is  0.11741959584887873  Accuracy :  0.9739233504543658 . Time taken :  89.43639063835144\n",
      "Test_loss at iter :  2  is  0.38909091671846774  Accuracy :  0.886635732048094 . Time taken :  7.466298818588257\n",
      "Train_loss at iter :  3  is  0.09210035231969706  Accuracy :  0.9811803030180859 . Time taken :  83.35605072975159\n",
      "Test_loss at iter :  3  is  0.4200089200420039  Accuracy :  0.8844309995641808 . Time taken :  7.807456016540527\n",
      "Train_loss at iter :  4  is  0.07256802750115936  Accuracy :  0.986078745998597 . Time taken :  64.21053552627563\n",
      "Test_loss at iter :  4  is  0.4324590240117847  Accuracy :  0.8853026379880534 . Time taken :  5.939998149871826\n",
      "Train_loss at iter :  5  is  0.05756722131351144  Accuracy :  0.9900337851458244 . Time taken :  58.75729775428772\n",
      "Test_loss at iter :  5  is  0.45928956730863835  Accuracy :  0.8859948214423052 . Time taken :  5.8524253368377686\n",
      "Train_loss at iter :  6  is  0.04520944792186662  Accuracy :  0.9925132439384288 . Time taken :  55.35183787345886\n",
      "Test_loss at iter :  6  is  0.47295151008780817  Accuracy :  0.8834311790191504 . Time taken :  5.98894739151001\n",
      "Train_loss at iter :  7  is  0.03493987550517777  Accuracy :  0.9946661398656658 . Time taken :  52.05617570877075\n",
      "Test_loss at iter :  7  is  0.49864463172683954  Accuracy :  0.8848924552003486 . Time taken :  5.83683443069458\n",
      "Train_loss at iter :  8  is  0.02734480369577314  Accuracy :  0.9961860682637338 . Time taken :  49.13904070854187\n",
      "Test_loss at iter :  8  is  0.5347140963975366  Accuracy :  0.8816366293229421 . Time taken :  6.054393768310547\n",
      "Train_loss at iter :  9  is  0.02081854356929031  Accuracy :  0.997371370515808 . Time taken :  47.014843702316284\n",
      "Test_loss at iter :  9  is  0.5264600254710838  Accuracy :  0.8889686466531648 . Time taken :  5.97219181060791\n",
      "Train_loss at iter :  10  is  0.015919519267229357  Accuracy :  0.9981494770962513 . Time taken :  47.163870096206665\n",
      "Test_loss at iter :  10  is  0.5465830746611896  Accuracy :  0.8912502884097726 . Time taken :  5.909654140472412\n",
      "Train_loss at iter :  11  is  0.012011437526078443  Accuracy :  0.9987340649416622 . Time taken :  43.46549844741821\n",
      "Test_loss at iter :  11  is  0.6074655415030374  Accuracy :  0.8859435485938422 . Time taken :  5.849156618118286\n",
      "Train_loss at iter :  12  is  0.009552527131856592  Accuracy :  0.9990203114038978 . Time taken :  42.657644510269165\n",
      "Test_loss at iter :  12  is  0.5684633068705032  Accuracy :  0.893326838772528 . Time taken :  5.827872037887573\n",
      "Train_loss at iter :  13  is  0.0076369370624371005  Accuracy :  0.9992702731033148 . Time taken :  41.81924748420715\n",
      "Test_loss at iter :  13  is  0.6533663112534664  Accuracy :  0.8886866459866178 . Time taken :  5.84708833694458\n",
      "Train_loss at iter :  14  is  0.006293589247225704  Accuracy :  0.9994355703561552 . Time taken :  41.22546887397766\n",
      "Test_loss at iter :  14  is  0.633050354533076  Accuracy :  0.8918655625913298 . Time taken :  5.88614559173584\n",
      "Train_loss at iter :  15  is  0.005564047550326328  Accuracy :  0.9995000766011659 . Time taken :  40.94746947288513\n",
      "Test_loss at iter :  15  is  0.6528551773317537  Accuracy :  0.8920962904094137 . Time taken :  5.836885929107666\n",
      "Train_loss at iter :  16  is  0.0045244850107816065  Accuracy :  0.9996008676089954 . Time taken :  40.32452154159546\n",
      "Test_loss at iter :  16  is  0.6338006307428  Accuracy :  0.8958135719229882 . Time taken :  5.8414833545684814\n",
      "Train_loss at iter :  17  is  0.0045386429976626485  Accuracy :  0.9996331207315008 . Time taken :  40.54496622085571\n",
      "Test_loss at iter :  17  is  0.7362011680937487  Accuracy :  0.885610275078832 . Time taken :  5.859938383102417\n",
      "Train_loss at iter :  18  is  0.003747739484203474  Accuracy :  0.9997056902571381 . Time taken :  40.037843227386475\n",
      "Test_loss at iter :  18  is  0.6719915674867609  Accuracy :  0.8954546619837465 . Time taken :  5.822112083435059\n",
      "Train_loss at iter :  19  is  0.003258293161144679  Accuracy :  0.9997218168183908 . Time taken :  39.674922704696655\n",
      "Test_loss at iter :  19  is  0.6968714347098264  Accuracy :  0.8940446586510113 . Time taken :  5.85765528678894\n"
     ]
    }
   ],
   "source": [
    "# lets perform training\n",
    "print('Starting training ....')\n",
    "\n",
    "for i in range(20):\n",
    "    # randomly shuffle the training examples\n",
    "    random.shuffle(train)\n",
    "    trainLoss = 0\n",
    "    totalWords = 0\n",
    "    numCorr = 0\n",
    "    startTime = time.time()\n",
    "    \n",
    "    for sent,tags in train:\n",
    "        scores = computeScoresWithPrevTag(sent,tags)\n",
    "        loss = computeLoss(scores,tags)\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "        trainLoss += loss.value()\n",
    "        totalWords += len(sent)\n",
    "        numCorr += computeCorr(scores,tags)\n",
    "    print(\"Train_loss at iter : \",i,\" is \",trainLoss/totalWords,' Accuracy : ',(numCorr/totalWords),\". Time taken : \",(-startTime+time.time()))\n",
    "    \n",
    "    testLoss = 0\n",
    "    totalWords = 0\n",
    "    numCorr = 0\n",
    "    startTime = time.time()\n",
    "    \n",
    "    for sent,tags in test:\n",
    "        scores = computeScoresWithPrevTag(sent)\n",
    "        loss = computeLoss(scores,tags)\n",
    "        testLoss += loss.value()\n",
    "        totalWords += len(sent)\n",
    "        numCorr += computeCorr(scores,tags)\n",
    "    print(\"Test_loss at iter : \",i,\" is \",testLoss/totalWords,' Accuracy : ',(numCorr/totalWords),\". Time taken : \",(-startTime+time.time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
