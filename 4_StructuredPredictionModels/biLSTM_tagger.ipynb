{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------------\n",
    "## Summary : perform pos tagging using bidirectional LSTM\n",
    "## Author  : Srinivas Venkata Vemparala\n",
    "## Source  : https://github.com/neubig/nn4nlp-code\n",
    "##---------------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dynet as dy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets convert the words into integer\n",
    "# The default dictionary takes a function as input and outupts \n",
    "# it if key is not present in the map.\n",
    "w2i = defaultdict(lambda : len(w2i))\n",
    "t2i = defaultdict(lambda : len(t2i))\n",
    "\n",
    "# lets write a method to read the data from file\n",
    "def readData(fileName):\n",
    "    retList = []\n",
    "    \n",
    "    with open(fileName,'r+',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            words = []\n",
    "            tags = []\n",
    "            for wt in line.strip().split():\n",
    "                word,tag = wt.split('|')\n",
    "                words.append(w2i[word])\n",
    "                tags.append(t2i[tag])\n",
    "            retList.append([words,tags])\n",
    "    return retList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29002  :  10\n"
     ]
    }
   ],
   "source": [
    "# read the train data \n",
    "train = readData('../data/tags/train.txt')\n",
    "unk_word = w2i[\"<unk>\"]\n",
    "w2i = defaultdict(lambda: unk_word, w2i)\n",
    "unk_tag = t2i[\"<unk>\"]\n",
    "t2i = defaultdict(lambda: unk_tag, t2i)\n",
    "\n",
    "# read the test data\n",
    "test = readData('../data/tags/dev.txt')\n",
    "\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)\n",
    "print(nwords,' : ',ntags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets declare the embedding size and hidden layer size\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "\n",
    "# lets declare the model and trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# lets declare the parameters to the model\n",
    "W_emb = model.add_lookup_parameters((nwords,EMB_SIZE))\n",
    "\n",
    "# BiRNN\n",
    "lstm = dy.BiRNNBuilder(1,EMB_SIZE,HID_SIZE,model,dy.LSTMBuilder)\n",
    "\n",
    "# Word-level softmax\n",
    "W_sm = model.add_parameters((ntags, HID_SIZE))\n",
    "b_sm = model.add_parameters((ntags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets write a method to compute the scores\n",
    "def computeScores(sent):\n",
    "    # renew the cg\n",
    "    dy.renew_cg()\n",
    "    wSoftmax = dy.parameter(W_sm)\n",
    "    bSoftmax = dy.parameter(b_sm)\n",
    "        \n",
    "    # get the embeddings for the words\n",
    "    wordEmbs = [W_emb[x] for x in sent]\n",
    "    \n",
    "    # send the embeddings to the lstm\n",
    "    wordReps = lstm.transduce(wordEmbs)\n",
    "    \n",
    "    scores = [dy.affine_transform([bSoftmax,wSoftmax,wordRep]) for wordRep in wordReps]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now lets define method to compute the loss \n",
    "\n",
    "# Calculate MLE loss for one example\n",
    "def computeLoss(scores, tags):\n",
    "    losses = [dy.pickneglogsoftmax(score, tag) for score, tag in zip(scores, tags)]\n",
    "    return dy.esum(losses)\n",
    "\n",
    "# compute the number of correct classifications\n",
    "def computeCorr(scores,tags):\n",
    "    correct = [(np.argmax(score.npvalue())==tag) for score,tag in zip(scores,tags)]\n",
    "    return sum(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ....\n",
      "Train_loss at iter :  0  is  0.33780106687746864  Accuracy :  0.8997089155693886 . Time taken :  87.308180809021\n",
      "Test_loss at iter :  0  is  0.4073060695707017  Accuracy :  0.8711000589637757 . Time taken :  4.63921594619751\n",
      "Train_loss at iter :  1  is  0.17900118804154638  Accuracy :  0.9465404494472621 . Time taken :  78.09503149986267\n",
      "Test_loss at iter :  1  is  0.3669272116031442  Accuracy :  0.8911221062886149 . Time taken :  4.57405948638916\n",
      "Train_loss at iter :  2  is  0.12352797027011744  Accuracy :  0.9645699449277934 . Time taken :  77.88749408721924\n",
      "Test_loss at iter :  2  is  0.3790509220356239  Accuracy :  0.8948650242264209 . Time taken :  4.564040422439575\n",
      "Train_loss at iter :  3  is  0.0898828662547492  Accuracy :  0.9751005894258138 . Time taken :  76.16704511642456\n",
      "Test_loss at iter :  3  is  0.3820539796398534  Accuracy :  0.8995564898607942 . Time taken :  4.545998573303223\n",
      "Train_loss at iter :  4  is  0.06462492268438766  Accuracy :  0.982611535329264 . Time taken :  75.16895866394043\n",
      "Test_loss at iter :  4  is  0.44725356092485274  Accuracy :  0.8973773938011126 . Time taken :  4.532996416091919\n",
      "Train_loss at iter :  5  is  0.046274332496447225  Accuracy :  0.9878042880526371 . Time taken :  75.58400273323059\n",
      "Test_loss at iter :  5  is  0.5083655576104248  Accuracy :  0.8983003050734484 . Time taken :  4.534039735794067\n",
      "Train_loss at iter :  6  is  0.03303368595607499  Accuracy :  0.9913319733266677 . Time taken :  74.52846074104309\n",
      "Test_loss at iter :  6  is  0.5611223391496152  Accuracy :  0.8973004845284179 . Time taken :  4.406000375747681\n",
      "Train_loss at iter :  7  is  0.022926443199042554  Accuracy :  0.9941904063087108 . Time taken :  72.53751277923584\n",
      "Test_loss at iter :  7  is  0.621270906217103  Accuracy :  0.8978901222857436 . Time taken :  4.398002862930298\n",
      "Train_loss at iter :  8  is  0.015653214830563998  Accuracy :  0.9961860682637338 . Time taken :  72.5300452709198\n",
      "Test_loss at iter :  8  is  0.7310996685893074  Accuracy :  0.8914297433793934 . Time taken :  4.4079554080963135\n",
      "Train_loss at iter :  9  is  0.011098819017219957  Accuracy :  0.9972383263854732 . Time taken :  72.6223783493042\n",
      "Test_loss at iter :  9  is  0.7578589680634505  Accuracy :  0.8962237547106929 . Time taken :  4.403995037078857\n",
      "Train_loss at iter :  10  is  0.007290339794128484  Accuracy :  0.998141413815625 . Time taken :  72.55595922470093\n",
      "Test_loss at iter :  10  is  0.8449692002817525  Accuracy :  0.8919937447124875 . Time taken :  4.4310431480407715\n",
      "Train_loss at iter :  11  is  0.004788976804254989  Accuracy :  0.9988348559494916 . Time taken :  72.58253908157349\n",
      "Test_loss at iter :  11  is  0.9999965552069564  Accuracy :  0.893890840105622 . Time taken :  4.391001224517822\n",
      "Train_loss at iter :  12  is  0.0032828536347295012  Accuracy :  0.9991735137357985 . Time taken :  74.60691380500793\n",
      "Test_loss at iter :  12  is  0.9675661672719346  Accuracy :  0.89650575537724 . Time taken :  4.5549561977386475\n",
      "Train_loss at iter :  13  is  0.0022315097711224285  Accuracy :  0.999367032470831 . Time taken :  77.79912066459656\n",
      "Test_loss at iter :  13  is  1.0368144343907653  Accuracy :  0.8930704745302125 . Time taken :  4.885030746459961\n",
      "Train_loss at iter :  14  is  0.0015532466906456339  Accuracy :  0.9995928043283691 . Time taken :  77.89339423179626\n",
      "Test_loss at iter :  14  is  1.076770629416635  Accuracy :  0.8960186633168405 . Time taken :  4.666999816894531\n",
      "Train_loss at iter :  15  is  0.0011243379837260182  Accuracy :  0.9996976269765117 . Time taken :  77.11464548110962\n",
      "Test_loss at iter :  15  is  1.1713191097623785  Accuracy :  0.8952239341656626 . Time taken :  5.051914691925049\n",
      "Train_loss at iter :  16  is  0.0008543673781546439  Accuracy :  0.9997661648618357 . Time taken :  76.18505954742432\n",
      "Test_loss at iter :  16  is  1.1898918595188603  Accuracy :  0.8956341169533674 . Time taken :  4.707409381866455\n",
      "Train_loss at iter :  17  is  0.000808549622844441  Accuracy :  0.9997822914230884 . Time taken :  76.94030618667603\n",
      "Test_loss at iter :  17  is  1.2627831175619648  Accuracy :  0.8956853898018304 . Time taken :  4.604645252227783\n",
      "Train_loss at iter :  18  is  0.0005807105113383446  Accuracy :  0.9998387343874728 . Time taken :  83.09834480285645\n",
      "Test_loss at iter :  18  is  1.2647132205410592  Accuracy :  0.8968390288922501 . Time taken :  5.72400164604187\n",
      "Train_loss at iter :  19  is  0.0005411715706056505  Accuracy :  0.9998709875099783 . Time taken :  79.7763352394104\n",
      "Test_loss at iter :  19  is  1.3376596466183959  Accuracy :  0.8945317507114108 . Time taken :  5.228302717208862\n"
     ]
    }
   ],
   "source": [
    "# lets perform training\n",
    "print('Starting training ....')\n",
    "for i in range(20):\n",
    "    # randomly shuffle the training examples\n",
    "    random.shuffle(train)\n",
    "    trainLoss = 0\n",
    "    totalWords = 0\n",
    "    numCorr = 0\n",
    "    startTime = time.time()\n",
    "    \n",
    "    for sent,tags in train:\n",
    "        scores = computeScores(sent)\n",
    "        loss = computeLoss(scores,tags)\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "        trainLoss += loss.value()\n",
    "        totalWords += len(sent)\n",
    "        numCorr += computeCorr(scores,tags)\n",
    "    print(\"Train_loss at iter : \",i,\" is \",trainLoss/totalWords,' Accuracy : ',(numCorr/totalWords),\". Time taken : \",(-startTime+time.time()))\n",
    "    \n",
    "    testLoss = 0\n",
    "    totalWords = 0\n",
    "    numCorr = 0\n",
    "    startTime = time.time()\n",
    "    \n",
    "    for sent,tags in test:\n",
    "        scores = computeScores(sent)\n",
    "        loss = computeLoss(scores,tags)\n",
    "        testLoss += loss.value()\n",
    "        totalWords += len(sent)\n",
    "        numCorr += computeCorr(scores,tags)\n",
    "    print(\"Test_loss at iter : \",i,\" is \",testLoss/totalWords,' Accuracy : ',(numCorr/totalWords),\". Time taken : \",(-startTime+time.time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
